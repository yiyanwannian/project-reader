# DeepSeekMath GRPO 数学公式详解

本文档对 DeepSeekMath 论文中第4章"强化学习"部分的核心数学公式进行逐一讲解。

---

## 1. PPO 目标函数

$$\mathcal{J}_{\textit{PPO}}(\theta)=\mathbb{E}\left[q\sim P(Q),o\sim \pi_{\theta_{old}}(O|q)\right]\frac{1}{|o|}\sum_{t=1}^{|o|}\min\left[\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{old}}(o_{t}|q,o_{<t})}A_{t},\textrm{clip}\left(\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{old}}(o_{t}|q,o_{<t})},1-\epsilon,1+\epsilon\right)A_{t}\right]$$

### 符号解释

| 符号 | 含义 |
|------|------|
| $\theta$ | 当前策略模型的参数 |
| $\theta_{old}$ | 旧策略模型的参数（用于采样） |
| $q$ | 从问题分布 $P(Q)$ 中采样的问题 |
| $o$ | 模型生成的输出序列 |
| $o_t$ | 输出序列中第 $t$ 个 token |
| $o_{<t}$ | 第 $t$ 个 token 之前的所有 token |
| $\pi_{\theta}(o_t\|q,o_{<t})$ | 当前策略在给定问题和历史token下生成 $o_t$ 的概率 |
| $A_t$ | 第 $t$ 步的优势值（Advantage） |
| $\epsilon$ | 裁剪超参数，通常为 0.1-0.2 |

### 核心思想

1. **重要性采样比率**: $\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{old}}(o_{t}|q,o_{<t})}$ 衡量新旧策略的差异
2. **裁剪机制**: `clip(ratio, 1-ε, 1+ε)` 将比率限制在 $[1-\epsilon, 1+\epsilon]$ 范围内
3. **取最小值**: 防止策略更新过大，保证训练稳定性

### 详细解释

#### 为什么需要重要性采样？

在强化学习中，我们用旧策略 $\pi_{\theta_{old}}$ 采样数据，但要更新的是新策略 $\pi_{\theta}$。重要性采样比率解决了这个分布不匹配问题：

$$\text{ratio}_t = \frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{old}}(o_{t}|q,o_{<t})}$$

- 当 $\text{ratio} > 1$：新策略比旧策略**更倾向于**生成这个 token
- 当 $\text{ratio} < 1$：新策略比旧策略**更不倾向于**生成这个 token
- 当 $\text{ratio} = 1$：新旧策略对这个 token 的偏好相同

#### 裁剪机制的作用

$$\text{clip}(\text{ratio}, 1-\epsilon, 1+\epsilon) = \begin{cases} 1-\epsilon & \text{if ratio} < 1-\epsilon \\ \text{ratio} & \text{if } 1-\epsilon \leq \text{ratio} \leq 1+\epsilon \\ 1+\epsilon & \text{if ratio} > 1+\epsilon \end{cases}$$

**数值示例**（$\epsilon = 0.2$）：

| 原始 ratio | 裁剪后 | 解释 |
|------------|--------|------|
| 0.5 | 0.8 | 策略变化太大，被限制 |
| 0.9 | 0.9 | 在安全范围内，保持不变 |
| 1.0 | 1.0 | 无变化 |
| 1.1 | 1.1 | 在安全范围内，保持不变 |
| 1.5 | 1.2 | 策略变化太大，被限制 |

#### min 操作的精妙设计

取 min 的目的是创建一个**悲观的下界**：

- **当 $A_t > 0$（好动作）**: 我们想增加这个动作的概率，但 min 会阻止 ratio 增长过快
- **当 $A_t < 0$（坏动作）**: 我们想减少这个动作的概率，但 min 会阻止 ratio 下降过快

这确保了无论优势是正是负，策略更新都被限制在安全范围内。

---

## 2. KL 惩罚的奖励函数

$$r_{t}=r_{\varphi}(q,o_{\leq t})-\beta\log\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{ref}(o_{t}|q,o_{<t})}$$

### 符号解释

| 符号 | 含义 |
|------|------|
| $r_{\varphi}$ | 奖励模型（参数为 $\varphi$） |
| $\pi_{ref}$ | 参考模型（通常是初始 SFT 模型） |
| $\beta$ | KL 惩罚系数（论文中使用 0.04） |

### 核心思想

- 第一项 $r_{\varphi}(q,o_{\leq t})$: 奖励模型给出的原始奖励
- 第二项 $-\beta\log\frac{\pi_{\theta}}{\pi_{ref}}$: KL 散度惩罚项，防止策略偏离参考模型太远

### 详细解释

#### 为什么需要 KL 惩罚？

没有 KL 惩罚时，模型可能会找到奖励模型的"漏洞"（reward hacking），生成获得高分但实际上质量很差的输出。KL 惩罚强制模型保持在参考模型附近，防止过度优化。

#### 数学直觉

$$-\beta\log\frac{\pi_{\theta}}{\pi_{ref}} = \beta\log\frac{\pi_{ref}}{\pi_{\theta}}$$

- 当 $\pi_{\theta} > \pi_{ref}$（新策略更喜欢这个 token）：惩罚项为**负**，减少奖励
- 当 $\pi_{\theta} < \pi_{ref}$（新策略更不喜欢这个 token）：惩罚项为**正**，增加奖励
- 当 $\pi_{\theta} = \pi_{ref}$：惩罚项为 **0**

**数值示例**（$\beta = 0.04$）：

| $\pi_{\theta}$ | $\pi_{ref}$ | 惩罚项 | 解释 |
|----------------|-------------|--------|------|
| 0.8 | 0.2 | $-0.04 \times \ln(4) \approx -0.055$ | 偏离太多，受惩罚 |
| 0.3 | 0.3 | $0$ | 完全一致，无惩罚 |
| 0.1 | 0.4 | $+0.04 \times \ln(4) \approx +0.055$ | 回归参考，有奖励 |

#### $\beta$ 的权衡

- **$\beta$ 太小**：模型可能过度优化奖励，偏离参考模型太远
- **$\beta$ 太大**：模型几乎不更新，学不到新东西

---

## 3. GRPO 目标函数

$$\mathcal{J}_{GRPO}(\theta)=\mathbb{E}[q\sim p(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{old}}(O|q)] \frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_{i}|}\sum_{t=1}^{|o_{i}|}\left\{\min\left[\text{ratio}\cdot\hat{A}_{i,t},\text{clip}(\text{ratio})\cdot\hat{A}_{i,t}\right]-\beta\text{D}_{\text{KL}}\left[\pi_{\theta}||\pi_{ref}\right]\right\}$$

### 符号解释

| 符号 | 含义 |
|------|------|
| $G$ | 每个问题采样的输出数量（组大小） |
| $\{o_1, o_2, \cdots, o_G\}$ | 对同一问题采样的 G 个输出 |
| $\hat{A}_{i,t}$ | 第 $i$ 个输出第 $t$ 步的**相对优势** |

### GRPO 与 PPO 的核心区别

| 特性 | PPO | GRPO |
|------|-----|------|
| 价值函数 | 需要额外训练 Value Model | **不需要** |
| 基线计算 | 使用 Value Model 预测 | 使用**组内平均奖励** |
| KL 惩罚 | 加在每个 token 的奖励上 | **直接加在损失函数上** |

### 详细解释

#### GRPO 的创新点

**问题背景**：PPO 需要训练一个与策略模型规模相当的价值函数（Value Model），这带来两个问题：
1. **内存开销**：需要额外约 50% 的 GPU 内存
2. **训练困难**：在 LLM 场景下，通常只有最后一个 token 获得奖励，这使得训练准确的逐 token 价值函数非常困难

**GRPO 的解决方案**：用"组内比较"替代价值函数

#### 工作流程图解

```
问题 q
    ↓
旧策略 π_θ_old 采样 G 个输出
    ↓
[o₁, o₂, o₃, ..., oG]  (例如 G=64)
    ↓
奖励模型打分
    ↓
[r₁, r₂, r₃, ..., rG]
    ↓
组内标准化 → 相对优势 Â
    ↓
更新策略 π_θ
```

#### 为什么组内比较有效？

奖励模型通常是在**同一问题的输出比较数据**上训练的（例如："输出 A 比输出 B 好"）。GRPO 的组内比较方式与奖励模型的训练方式天然契合：

- 奖励模型擅长**相对比较**，而非给出绝对分数
- GRPO 只关心"哪个输出更好"，而非"这个输出有多好"

#### 论文中的超参数设置

| 超参数 | 值 | 含义 |
|--------|-----|------|
| $G$ | 64 | 每个问题采样的输出数量 |
| $\beta$ | 0.04 | KL 惩罚系数 |
| $\epsilon$ | 0.2（典型值） | 裁剪范围 |
| 学习率 | $1 \times 10^{-6}$ | 策略模型学习率 |
| 最大长度 | 1024 | 输出序列最大长度 |
| 批大小 | 1024 | 训练批次大小 |

---

## 4. KL 散度的无偏估计

$$\mathbb{D}_{KL}\left[\pi_{\theta}||\pi_{ref}\right]=\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}-\log\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}-1$$

### 核心思想

这是 Schulman (2020) 提出的无偏估计量。设 $r = \frac{\pi_{ref}}{\pi_{\theta}}$，则：

$$D_{KL} = r - \log r - 1$$

**性质**: 该估计量恒为非负（当 $r=1$ 时取得最小值 0）

### 详细解释

#### 为什么不直接用标准 KL 散度？

标准 KL 散度定义为：

$$D_{KL}(P||Q) = \mathbb{E}_{x \sim P}\left[\log \frac{P(x)}{Q(x)}\right]$$

但在实践中，我们需要用采样来估计这个期望。直接用 $\log \frac{\pi_{ref}}{\pi_{\theta}}$ 作为估计量是有偏的。

#### 无偏估计量的推导

设 $r = \frac{\pi_{ref}}{\pi_{\theta}}$，Schulman 提出的估计量为：

$$\hat{D}_{KL} = r - \log r - 1$$

**验证非负性**：令 $f(r) = r - \log r - 1$

- $f'(r) = 1 - \frac{1}{r}$
- 当 $r = 1$ 时，$f'(1) = 0$（极值点）
- $f''(r) = \frac{1}{r^2} > 0$（凹函数，极小值）
- $f(1) = 1 - 0 - 1 = 0$

因此 $f(r) \geq 0$ 对所有 $r > 0$ 成立，当且仅当 $r = 1$ 时取等号。

#### 数值示例

| $\pi_{\theta}$ | $\pi_{ref}$ | $r = \frac{\pi_{ref}}{\pi_{\theta}}$ | $\hat{D}_{KL} = r - \log r - 1$ |
|----------------|-------------|--------------------------------------|--------------------------------|
| 0.5 | 0.5 | 1.0 | 0 |
| 0.3 | 0.6 | 2.0 | $2 - 0.693 - 1 = 0.307$ |
| 0.6 | 0.3 | 0.5 | $0.5 - (-0.693) - 1 = 0.193$ |
| 0.1 | 0.9 | 9.0 | $9 - 2.197 - 1 = 5.803$ |

可以看到，偏离越大，KL 惩罚越大。

---

## 5. 结果监督的优势计算

$$\hat{A}_{i,t}=\overline{r}_{i}=\frac{r_{i}-\text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}$$

### 核心思想

- 对 G 个输出的奖励进行 **Z-score 标准化**
- 输出内所有 token 共享同一个优势值
- 奖励仅在序列末尾给出（结果奖励）

### 详细解释

#### Z-score 标准化的作用

假设对问题 $q$ 采样了 $G=5$ 个输出，奖励分别为：

$$\mathbf{r} = [0.2, 0.8, 0.5, 0.3, 0.7]$$

计算统计量：
- $\text{mean}(\mathbf{r}) = 0.5$
- $\text{std}(\mathbf{r}) = 0.228$

标准化后的优势：

| 输出 | 原始奖励 $r_i$ | 标准化优势 $\hat{A}_i$ | 解释 |
|------|---------------|------------------------|------|
| $o_1$ | 0.2 | $(0.2-0.5)/0.228 = -1.32$ | 最差，强惩罚 |
| $o_2$ | 0.8 | $(0.8-0.5)/0.228 = +1.32$ | 最好，强强化 |
| $o_3$ | 0.5 | $(0.5-0.5)/0.228 = 0$ | 平均水平，不变 |
| $o_4$ | 0.3 | $(0.3-0.5)/0.228 = -0.88$ | 较差，弱惩罚 |
| $o_5$ | 0.7 | $(0.7-0.5)/0.228 = +0.88$ | 较好，弱强化 |

#### 为什么用标准化而非原始奖励？

1. **消除奖励尺度的影响**：不同问题的奖励分布可能差异很大
2. **平衡正负样本**：标准化后，一半样本被强化，一半被惩罚
3. **与奖励模型的比较性质匹配**：奖励模型擅长比较，而非绝对评分

---

## 6. 过程监督的优势计算

$$\hat{A}_{i,t}=\sum_{index(j)\geq t}\overline{r}_{i}^{index(j)}$$

### 符号解释

| 符号 | 含义 |
|------|------|
| $index(j)$ | 第 $j$ 个推理步骤的结束 token 索引 |
| $K_i$ | 第 $i$ 个输出的总步骤数 |
| $\overline{r}_{i}^{index(j)}$ | 标准化后的第 $j$ 步奖励 |

### 核心思想

- 每个推理步骤末尾都有奖励（过程奖励模型 PRM）
- 某个 token 的优势 = **后续所有步骤奖励之和**
- 提供更密集的监督信号，适合复杂数学推理

### 详细解释

#### 结果监督 vs 过程监督

```
数学推理示例：
"计算 (2+3) × 4 = ?"

步骤1: 2+3 = 5       ← 过程奖励 r¹
步骤2: 5 × 4 = 20    ← 过程奖励 r²
最终答案: 20         ← 结果奖励 R
```

| 监督方式 | 奖励位置 | 优势计算 |
|----------|----------|----------|
| 结果监督 | 仅最后 | 所有 token 共享 $R$ |
| 过程监督 | 每步结尾 | token $t$ 获得 $\sum_{j \geq t} r^j$ |

#### 数值示例

假设一个输出有 3 个推理步骤，标准化后的步骤奖励为：

$$\overline{r}^{(1)} = +0.5, \quad \overline{r}^{(2)} = -0.3, \quad \overline{r}^{(3)} = +0.8$$

各步骤内 token 的优势：

| token 位置 | 优势 $\hat{A}$ | 计算方式 |
|------------|----------------|----------|
| 步骤1内 | $0.5 + (-0.3) + 0.8 = 1.0$ | 累加后续所有步骤 |
| 步骤2内 | $(-0.3) + 0.8 = 0.5$ | 累加步骤2和3 |
| 步骤3内 | $0.8$ | 仅步骤3 |

#### 为什么过程监督更有效？

1. **信用分配更精确**：每一步都有反馈，而非等到最后
2. **梯度信号更密集**：避免长序列中梯度消失
3. **适合数学推理**：数学题每一步都可以验证对错

#### 过程监督的挑战

- 需要训练**过程奖励模型（PRM）**，成本较高
- 需要将输出切分为有意义的"步骤"，需要额外处理

---

## 7. 统一范式的梯度公式（5.2.1 节）

$$\nabla_{\theta}\mathcal{J}_{\mathcal{A}}(\theta)=\mathbb{E}[(\underbrace{q,o}_{Data\ Source})\sim\mathcal{D}]\left(\frac{1}{|o|}\sum_{t=1}^{|o|}\underbrace{GC_{\mathcal{A}}(q,o,t,\pi_{rf})}_{Gradient\ Coefficient}\nabla_{\theta}\log\pi_{\theta}(o_{t}|q,o_{<t})\right)$$

### 符号解释

| 符号 | 含义 |
|------|------|
| $\nabla_{\theta}\mathcal{J}_{\mathcal{A}}(\theta)$ | 算法 $\mathcal{A}$ 的目标函数对参数 $\theta$ 的梯度 |
| $\mathcal{D}$ | 数据源分布 |
| $(q, o)$ | 问题-输出对 |
| $GC_{\mathcal{A}}(q,o,t,\pi_{rf})$ | **梯度系数**，由算法 $\mathcal{A}$ 根据奖励函数 $\pi_{rf}$ 计算 |
| $\nabla_{\theta}\log\pi_{\theta}(o_{t}\|q,o_{<t})$ | 策略梯度的核心项（对数概率的梯度） |

### 核心思想

这个公式揭示了**所有训练方法的统一形式**，包含三个关键组成部分：

| 组成部分 | 含义 | 作用 |
|----------|------|------|
| **数据源** $\mathcal{D}$ | 训练数据的来源 | 决定从哪里采样 $(q, o)$ |
| **奖励函数** $\pi_{rf}$ | 训练信号的来源 | 提供奖励/反馈信号 |
| **梯度系数** $GC$ | 算法的核心差异 | 决定强化或惩罚的幅度 |

### 不同方法的统一视角

| 方法 | 数据源 | 梯度系数特点 |
|------|--------|--------------|
| **SFT** | 人工标注数据（离线） | 恒为 1（均匀强化） |
| **RFT** | SFT 模型采样 + 过滤（离线） | 正确答案为 1，错误为 0 |
| **DPO** | SFT 模型采样（离线） | 基于偏好对的对比损失 |
| **Online RFT** | 实时策略采样（在线） | 正确答案为 1，错误为 0 |
| **GRPO** | 实时策略采样（在线） | **根据奖励值差异化调整** |

### 详细解释

#### 策略梯度的本质

公式的核心项是 $\nabla_{\theta}\log\pi_{\theta}(o_{t}|q,o_{<t})$，这来源于策略梯度定理（Policy Gradient Theorem）：

$$\nabla_{\theta} J(\theta) = \mathbb{E}\left[R \cdot \nabla_{\theta}\log\pi_{\theta}(a|s)\right]$$

**直觉理解**：
- $\nabla_{\theta}\log\pi_{\theta}$ 指向"增加该动作概率"的方向
- 乘以正的 $GC$（梯度系数）：沿此方向更新，**增加**该动作概率
- 乘以负的 $GC$：沿反方向更新，**减少**该动作概率

#### 各方法的梯度系数对比

**SFT（监督微调）**：
$$GC_{SFT} = 1$$
对所有人工标注的样本均匀强化，不区分质量好坏。

**RFT（拒绝采样微调）**：
$$GC_{RFT} = \mathbb{1}[\text{答案正确}]$$
只保留正确答案的样本进行训练，错误样本直接丢弃（梯度系数为 0）。

**Online RFT**：
$$GC_{OnlineRFT} = \mathbb{1}[\text{答案正确}]$$
与 RFT 类似，但数据来自实时策略而非固定的 SFT 模型。

**GRPO**：
$$GC_{GRPO} = \hat{A}_{i,t} \cdot \frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{old}}(o_{t}|q,o_{<t})}$$
梯度系数是连续值，取决于：
1. 相对优势 $\hat{A}_{i,t}$：该输出比组内平均好多少
2. 重要性采样比率：新旧策略的差异

#### 数值示例

假设对问题 $q$ 采样了 3 个输出：

| 输出 | 答案正确? | 奖励 | Online RFT 的 $GC$ | GRPO 的 $GC$（假设 ratio=1） |
|------|-----------|------|-------------------|------------------------------|
| $o_1$ | ✅ | 0.9 | 1 | +1.2（强强化） |
| $o_2$ | ✅ | 0.6 | 1 | +0.3（弱强化） |
| $o_3$ | ❌ | 0.2 | 0 | -1.5（强惩罚） |

**关键区别**：
- Online RFT 对 $o_1$ 和 $o_2$ 同等强化，无法区分质量差异
- GRPO 对 $o_1$ 的强化力度是 $o_2$ 的 4 倍
- Online RFT 忽略 $o_3$，GRPO 主动惩罚它

#### 在线 vs 离线采样的差异

```
训练进度 ──────────────────────────────────────►
                    │
离线采样（RFT/DPO）  │  始终使用初始 SFT 模型的采样数据
                    │  → 数据与当前策略越来越不匹配
                    │
在线采样（GRPO）     │  始终使用当前策略的采样数据
                    │  → 数据与策略保持一致，探索更有效
```

论文实验表明，在训练后期，在线方法（Online RFT、GRPO）的性能显著超过离线方法（RFT、DPO）。

### 关键洞察

1. **在线 vs 离线**: 在线采样（Online RFT、GRPO）在训练后期显著优于离线采样（RFT、DPO）
2. **差异化梯度系数**: GRPO 的优势在于能够根据奖励值的大小进行**差异化的强化和惩罚**，而非简单的二值判断
3. **细粒度监督**: 过程监督（GRPO+PS）优于结果监督（GRPO+OS），表明步骤级梯度系数更有效

---

## 8. RL 有效性的解释（5.2.2 节）

论文通过对比 Pass@K 和 Maj@K 指标揭示了 RL 有效的本质原因：

| 指标 | 定义 | RL 后变化 |
|------|------|----------|
| **Pass@K** | K 次采样中至少有一个正确的概率 | ❌ 未提升 |
| **Maj@K** | K 次采样中多数投票的准确率 | ✅ 显著提升 |

### 详细解释

#### Pass@K 和 Maj@K 的区别

**Pass@K**：衡量模型的"上限能力"
- 采样 K 次，只要有一次正确就算通过
- 反映模型是否"有可能"解决问题
- 公式：$P(\text{至少一次正确}) = 1 - P(\text{全部错误})$

**Maj@K**：衡量模型的"稳定性"
- 采样 K 次，取多数投票的结果
- 反映模型是否"稳定地"给出正确答案
- 需要超过半数的采样结果正确

#### 数值示例

假设对某个问题采样 10 次：

| 模型 | 10 次采样结果 | Pass@10 | Maj@10 |
|------|--------------|---------|--------|
| SFT 模型 | ✅❌❌❌❌❌❌❌❌❌ (1正9错) | ✅ 通过 | ❌ 失败 |
| RL 模型 | ✅✅✅✅✅✅❌❌❌❌ (6正4错) | ✅ 通过 | ✅ 通过 |

- Pass@10：两个模型都通过（因为都至少有一次正确）
- Maj@10：只有 RL 模型通过（因为正确次数超过半数）

#### 这说明了什么？

```
SFT 模型的问题：

输出分布
    ▲
    │    ████████████████████  ← 错误答案（高概率）
    │  ██                      ← 正确答案（低概率）
    └────────────────────────► 答案空间

RL 后的改善：

输出分布
    ▲
    │  ████████████████████    ← 正确答案（高概率）
    │        ████████          ← 错误答案（低概率）
    └────────────────────────► 答案空间
```

**核心洞察**：RL 没有教会模型新知识，而是让模型更"自信"地输出它已经知道的正确答案。

#### 为什么会这样？

1. **SFT 的局限性**：SFT 训练时，模型可能学到了正确答案的模式，但没有学会"优先"输出它
2. **RL 的作用**：通过奖励信号，强化正确答案的输出概率，抑制错误答案
3. **本质是对齐**：解决的是"知道但不说"的问题，而非"不知道"的问题

### 核心结论

> **RL 的改进归因于使输出分布更加稳健（从 TopK 中提升正确答案的概率），而非增强模型的基础能力。**

换句话说：
- RL **没有**让模型学会解决之前完全不会的问题
- RL **确实**让模型更稳定地输出正确答案，减少了"会但答错"的情况

这解释了为什么 RL 在 SFT 后仍然有效——它解决的是 SFT 模型的**错位问题**（misalignment），而非能力问题。

#### 实际意义

| 如果你想... | 应该用... |
|-------------|-----------|
| 让模型学会新知识/能力 | 更多的预训练数据、更好的 SFT 数据 |
| 让模型更稳定地输出正确答案 | **RL（如 GRPO）** |
| 提升 Pass@K（探索上限） | 更好的采样策略、更大的模型 |
| 提升 Maj@K（稳定性） | **RL（如 GRPO）** |

---

## 总结：GRPO 的优势

1. **计算高效**: 无需训练额外的 Value Model，节省约 50% 内存
2. **更稳定**: 使用组内相对奖励作为基线，与奖励模型的比较训练方式天然契合
3. **灵活性**: 同时支持结果监督（Outcome）和过程监督（Process）
4. **统一范式**: 可以用统一的梯度公式理解，核心差异在于梯度系数的计算方式
5. **差异化强化**: 能根据奖励值大小差异化地强化/惩罚输出，优于二值判断方法
6. **在线学习**: 使用实时策略采样，比离线方法更能适应策略变化

