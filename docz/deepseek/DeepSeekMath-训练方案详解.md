# DeepSeekMath 训练方案详解

本文档详细讲解 DeepSeekMath 论文中的训练处理方案和完整训练流程。

---

## 训练流程总览

DeepSeekMath 的训练分为三个主要阶段：

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        DeepSeekMath 训练流程                              │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐              │
│  │   阶段 1     │ → │   阶段 2     │ → │   阶段 3     │              │
│  │  数学预训练   │    │  监督微调    │    │  强化学习    │              │
│  │  (500B tokens)│    │  (SFT)      │    │  (GRPO)     │              │
│  └──────────────┘    └──────────────┘    └──────────────┘              │
│         ↓                   ↓                   ↓                      │
│  DeepSeekMath-Base   DeepSeekMath-Instruct  DeepSeekMath-RL           │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 阶段 0：基础模型选择

### 起点：DeepSeek-Coder-Base-v1.5 7B

DeepSeekMath 没有从零开始训练，而是选择 **DeepSeek-Coder-Base-v1.5 7B** 作为初始化模型。

#### 为什么选择代码模型而非通用模型？

论文通过实验验证了一个重要发现：

> **代码训练能够提升数学推理能力**

| 训练路径 | GSM8K (CoT) | MATH (CoT) | GSM8K (PoT) | MATH (PoT) |
|----------|-------------|------------|-------------|------------|
| 直接数学训练 | 26.5% | 11.9% | 29.4% | 10.3% |
| 代码→数学训练 | **32.4%** | **14.9%** | **44.3%** | **16.6%** |
| 通用→数学训练 | 28.7% | 11.7% | 32.0% | 10.7% |

- **CoT**: 思维链推理（不使用工具）
- **PoT**: 程序链推理（使用 Python）

**结论**：先进行代码训练，再进行数学训练，效果最佳。

---

## 阶段 1：数学预训练

### 1.1 数据收集：DeepSeekMath 语料库

#### 迭代式数据收集流程

```
┌─────────────────────────────────────────────────────────────────────┐
│                    迭代数据收集流程                                    │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  第1轮迭代:                                                          │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐             │
│  │ OpenWebMath │ → │ 训练fastText │ → │ 召回数学网页 │ → 40B tokens │
│  │  (种子语料)  │    │   分类器    │    │  (第1批)    │             │
│  └─────────────┘    └─────────────┘    └─────────────┘             │
│         ↓                                                           │
│  第2-4轮迭代:                                                        │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐             │
│  │ 人工标注新的 │ → │ 更新fastText │ → │ 召回更多网页 │ → 更多tokens │
│  │  数学域URL   │    │   分类器    │    │             │             │
│  └─────────────┘    └─────────────┘    └─────────────┘             │
│                                                                      │
│  最终结果: 35.5M 数学网页, 120B tokens                               │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

#### fastText 分类器配置

| 参数 | 值 |
|------|-----|
| 向量维度 | 256 |
| 学习率 | 0.1 |
| 单词 n-gram 最大长度 | 3 |
| 单词最小出现次数 | 3 |
| 训练轮数 | 3 |
| 正样本数 | 500,000（来自种子语料） |
| 负样本数 | 500,000（来自 Common Crawl） |

#### 域发现策略

1. 将 Common Crawl 按域（基础 URL）分组
2. 计算每个域被第一轮收集的网页比例
3. 超过 10% 被收集的域标记为"数学相关域"（如 mathoverflow.net）
4. 人工标注这些域内的数学相关 URL（如 mathoverflow.net/questions）
5. 将未收集的相关网页加入种子语料

### 1.2 数据去污染

为避免基准污染，采用严格的过滤规则：

| 过滤规则 | 条件 |
|----------|------|
| 10-gram 匹配 | 文本包含与基准完全匹配的 10-gram 字符串 → 移除 |
| 短文本精确匹配 | 长度 3-10 字符的基准文本 → 精确匹配后移除 |

涉及的基准：GSM8K、MATH、CMATH、AGIEval 等

### 1.3 预训练数据配比

| 数据来源 | 占比 | Token 数 |
|----------|------|----------|
| DeepSeekMath 语料库 | 56% | 280B |
| GitHub 代码 | 20% | 100B |
| arXiv | 10% | 50B |
| AlgebraicStack | 4% | 20B |
| 自然语言（中英文） | 10% | 50B |
| **总计** | **100%** | **500B** |

### 1.4 预训练超参数

| 超参数 | 值 |
|--------|-----|
| 优化器 | AdamW ($\beta_1=0.9, \beta_2=0.95$) |
| 权重衰减 | 0.1 |
| 最大学习率 | $4.2 \times 10^{-4}$ |
| 批大小 | 10M tokens |
| 上下文长度 | 4K |
| 学习率调度 | 多步调度（见下图） |

```
学习率调度:
     ▲
 峰值 │    ┌────────────────────┐
     │   /                      \
31.6%│  /                        \────────┐
     │ /                                   \
 10% │/                                     \────
     └─────────────────────────────────────────────► 训练进度
       ↑          ↑              ↑          ↑
     预热     达到峰值         80%        90%
    2000步
```

---

## 阶段 2：监督微调 (SFT)

### 2.1 SFT 数据构成

总样本数：**776K**

| 数据类型 | 描述 |
|----------|------|
| 英文 GSM8K/MATH | 工具集成解决方案标注 |
| MathInstruct 子集 | CoT 或 PoT 格式 |
| Lila-OOD 训练集 | CoT 或 PoT 格式 |
| 中文 K-12 数学 | 76 个子主题，CoT + 工具集成格式 |

### 2.2 解决方案格式

模型被训练生成三种格式的解答：

**1. 思维链 (CoT) - Chain of Thought**
```
问题：小明有5个苹果，小红给了他3个，请问小明现在有几个苹果？

解答：
让我们一步步思考：
1. 小明原来有 5 个苹果
2. 小红给了他 3 个苹果
3. 所以小明现在有 5 + 3 = 8 个苹果

答案：8
```

**2. 程序链 (PoT) - Program of Thought**
```python
# 小明原来的苹果数
original = 5
# 小红给的苹果数
given = 3
# 计算总数
total = original + given
print(total)  # 输出: 8
```

**3. 工具集成推理 (Tool-Integrated)**
```
问题：计算 sin(30°) + cos(60°)

解答：
我需要计算三角函数的值。让我使用 Python 来精确计算：

```python
import math
result = math.sin(math.radians(30)) + math.cos(math.radians(60))
print(result)
```

执行结果：1.0

因此，sin(30°) + cos(60°) = 1.0

### 2.3 SFT 训练超参数

| 超参数 | 值 |
|--------|-----|
| 基础模型 | DeepSeekMath-Base 7B |
| 最大上下文长度 | 4K tokens |
| 批大小 | 256 |
| 学习率 | $5 \times 10^{-5}$（恒定） |
| 训练步数 | 500 |
| 数据处理 | 随机连接样本至最大长度 |

---

## 阶段 3：强化学习 (GRPO)

### 3.1 GRPO 算法概述

**GRPO（Group Relative Policy Optimization）** 是论文提出的核心创新，相比 PPO 的优势：

| 特性 | PPO | GRPO |
|------|-----|------|
| 价值模型 | 需要（与策略模型同规模） | **不需要** |
| 内存开销 | 高（2x 模型内存） | **低（1x 模型内存）** |
| 基线计算 | Value Model 预测 | 组内平均奖励 |
| KL 惩罚 | 加在每个 token 奖励上 | 直接加在损失函数上 |

### 3.2 GRPO 工作流程

```
┌─────────────────────────────────────────────────────────────────────┐
│                        GRPO 训练循环                                  │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  对于每个问题 q:                                                      │
│                                                                      │
│  1. 采样阶段                                                         │
│     ┌─────────────┐                                                 │
│     │ 旧策略 π_old │ ──采样 G=64 个输出──→ {o₁, o₂, ..., o₆₄}       │
│     └─────────────┘                                                 │
│                                                                      │
│  2. 评分阶段                                                         │
│     ┌─────────────┐                                                 │
│     │  奖励模型   │ ──对每个输出评分──→ {r₁, r₂, ..., r₆₄}          │
│     └─────────────┘                                                 │
│                                                                      │
│  3. 标准化阶段                                                       │
│     对奖励进行 Z-score 标准化:                                        │
│     r̄ᵢ = (rᵢ - mean(r)) / std(r)                                   │
│                                                                      │
│  4. 优势计算                                                         │
│     • 结果监督: Â = r̄ᵢ (所有 token 共享)                            │
│     • 过程监督: Â = Σ(后续步骤的奖励)                                 │
│                                                                      │
│  5. 策略更新                                                         │
│     最大化 GRPO 目标函数，更新策略 π_θ                                │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 3.3 RL 训练数据

| 项目 | 详情 |
|------|------|
| 数据来源 | SFT 数据中与 GSM8K 和 MATH 相关的问题 |
| 格式 | 思维链 (CoT) |
| 问题数量 | ~144K |
| 设计意图 | 仅使用部分数据，研究 RL 在缺乏训练数据的基准上的泛化能力 |

### 3.4 奖励模型训练

| 超参数 | 值 |
|--------|-----|
| 基础模型 | DeepSeekMath-Base 7B |
| 学习率 | $2 \times 10^{-5}$ |
| 训练数据构建 | 遵循 Wang et al. (2023b) 的方法 |

### 3.5 GRPO 训练超参数

| 超参数 | 值 | 说明 |
|--------|-----|------|
| 基础模型 | DeepSeekMath-Instruct 7B | SFT 后的模型 |
| 策略学习率 | $1 \times 10^{-6}$ | 较小以保证稳定 |
| KL 系数 $\beta$ | 0.04 | 控制与参考模型的偏离 |
| 采样数 G | 64 | 每个问题采样的输出数 |
| 最大长度 | 1024 | 输出序列最大长度 |
| 批大小 | 1024 | |
| 更新频率 | 每个探索阶段后单次更新 | |

### 3.6 两种监督方式对比

#### 结果监督 (Outcome Supervision)

```
问题 → 模型生成完整解答 → 最后评分 → 整个序列共享同一奖励

优势: 简单，只需最终答案正确性
劣势: 信号稀疏，长序列梯度传播困难
```

#### 过程监督 (Process Supervision)

```
问题 → 模型生成解答 → 每一步都评分 → 每个 token 获得累积奖励

步骤1: 2+3=5     → 奖励 r¹
步骤2: 5×4=20    → 奖励 r²
步骤3: 答案是20  → 奖励 r³

token 的优势 = 后续所有步骤奖励之和
```

论文实验表明：**过程监督 > 结果监督**

### 3.7 迭代强化学习

随着训练进行，奖励模型可能无法有效监督不断进步的策略模型。迭代 RL 解决这个问题：

```
┌─────────────────────────────────────────────────────────────────────┐
│                        迭代 RL 流程                                   │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  第1轮:                                                              │
│  ┌─────────┐    ┌─────────┐    ┌─────────┐                         │
│  │ 策略模型 │ → │ 采样输出 │ → │ 更新奖励 │                         │
│  │  π₀     │    │         │    │  模型   │                         │
│  └─────────┘    └─────────┘    └─────────┘                         │
│       ↓                              ↓                              │
│  ┌─────────┐                   ┌─────────┐                         │
│  │ GRPO训练 │ ←──使用新奖励模型──│ RM₁    │                         │
│  └─────────┘                   └─────────┘                         │
│       ↓                                                             │
│  第2轮:                                                              │
│  ┌─────────┐                                                        │
│  │ 策略模型 │ → 重复上述过程...                                      │
│  │  π₁     │                                                        │
│  └─────────┘                                                        │
│                                                                      │
│  回放机制: 新奖励模型训练时保留 10% 历史数据                           │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 训练结果对比

### 各阶段性能提升

| 模型 | GSM8K | MATH | CMATH |
|------|-------|------|-------|
| DeepSeekMath-Base 7B | 64.2% | 36.2% | 70.1% |
| DeepSeekMath-Instruct 7B | 82.9% | 46.8% | 84.6% |
| DeepSeekMath-RL 7B | **88.2%** | **51.7%** | **88.8%** |

### 提升幅度

| 阶段 | GSM8K 提升 | MATH 提升 |
|------|-----------|-----------|
| 预训练 → SFT | +18.7% | +10.6% |
| SFT → RL | +5.3% | +4.9% |

---

## 关键经验总结

### 预训练阶段

1. **代码训练有益于数学推理**：先训练代码，再训练数学，效果最佳
2. **arXiv 论文效果有限**：对数学推理基准提升不明显
3. **数据规模和质量同等重要**：120B 高质量 tokens 优于更大规模的低质量数据

### SFT 阶段

1. **多格式训练**：同时训练 CoT、PoT、工具集成三种格式
2. **多语言覆盖**：中英文数学问题都要覆盖
3. **多难度级别**：从小学到大学水平都要涉及

### RL 阶段

1. **GRPO 优于 PPO**：节省内存，效果相当或更好
2. **在线优于离线**：实时采样比固定数据效果好
3. **过程监督优于结果监督**：细粒度反馈更有效
4. **迭代 RL 进一步提升**：动态更新奖励模型

### RL 的本质作用

> RL 没有教会模型新知识，而是让模型更稳定地输出它已经知道的正确答案。

- **Pass@K 未提升**：模型的"能力上限"没变
- **Maj@K 显著提升**：模型的"输出稳定性"大幅提高

这表明 RL 解决的是 SFT 模型的**对齐问题（misalignment）**，而非能力问题。

