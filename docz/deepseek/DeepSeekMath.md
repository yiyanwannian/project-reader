# DeepSeekMath：突破开源语言模型数学推理的极限

邵志宏^{1,2}，王沛毅^{1,3}，朱启昊^{1,3}，徐润新^{1}，宋俊骁^{1}
毕啸^{1}，张浩伟^{1}，张明川^{1}，Y.K. Li^{1}，Y. Wu^{1}，郭达亚^{1}1
^{1}深度求索人工智能，^{2}清华大学，^{3}北京大学
{zhihongshao,wangpeiyi,zhugh,guoday}@deepseek.com
https://github.com/deepseek-ai/DeepSeek-Math

1 通讯作者。

###### 摘要

数学推理因其复杂和结构化的特性，对语言模型构成了重大挑战。本文介绍了DeepSeekMath 7B，该模型在DeepSeek-Coder-Base-v1.5 7B的基础上，使用从Common Crawl获取的120B数学相关token，以及自然语言和代码数据进行持续预训练。在不依赖外部工具包和投票技术的情况下，DeepSeekMath 7B在竞赛级MATH基准测试中取得了51.7%的优异成绩，接近Gemini-Ultra和GPT-4的性能水平。对64个样本进行自洽性检验后，DeepSeekMath 7B在MATH上的得分达到60.9%。DeepSeekMath的数学推理能力归功于两个关键因素：首先，我们通过精心设计的数据选择流程，充分利用了公开可用的网络数据的巨大潜力。其次，我们引入了群组相对策略优化（GRPO），这是近端策略优化（PPO）的一个变体，它在增强数学推理能力的同时，优化了PPO的内存使用。

+
† 在深度求索人工智能实习期间完成的工作。

## 1 引言

大语言模型（LLM）已经彻底改变了人工智能处理数学推理的方法，并在定量推理基准（Hendrycks等人，2021）和几何推理基准（Trinh等人，2024）上推动了显著进步。此外，这些模型已被证明在协助人类解决复杂数学问题方面具有重要作用（Tao, 2023）。然而，像GPT-4（OpenAI, 2023）和Gemini-Ultra（Anil等人，2023）这样的尖端模型并未公开，目前可访问的开源模型在性能上明显落后。

在本研究中，我们介绍了DeepSeekMath，这是一个特定领域语言模型，其数学能力显著优于开源模型，并在学术基准上接近GPT-4的水平。为此，我们创建了DeepSeekMath语料库，这是一个包含120B数学token的大规模高质量预训练语料库。该数据集使用基于fastText的分类器（Joulin等人，2016）从Common Crawl（CC）中提取。在初始迭代中，使用OpenWebMath（Paster等人，2023）的实例作为正样本训练分类器，同时纳入多样化的其他网页作为负样本。随后，我们使用该分类器从CC中挖掘额外的正样本，并通过人工标注进一步精炼。然后用这个增强的数据集更新分类器以提高其性能。评估结果表明，大规模语料库质量很高，因为我们的基础模型DeepSeekMath-Base 7B在GSM8K（Cobbe等人，2021）上达到64.2%，在竞赛级MATH数据集（Hendrycks等人，2021）上达到36.2%，优于Minerva 540B（Lewkowycz等人，2022a）。此外，DeepSeekMath语料库是多语言的，因此我们注意到其在中文数学基准（Wei等人，2023; Zhong等人，2023）上的改进。我们相信，我们在数学数据处理方面的经验是研究社区的起点，未来有巨大的改进空间。

DeepSeekMath-Base使用DeepSeek-Coder-Base-v1.5 7B（Guo等人，2024）初始化，因为我们注意到从代码训练模型开始相比通用LLM是更好的选择。此外，我们观察到数学训练也提升了模型在MMLU（Hendrycks等人，2020）和BBH基准（Suzgun等人，2022）上的能力，表明它不仅增强了模型的数学能力，还放大了通用推理能力。

预训练之后，我们使用思维链（Wei等人，2022）、程序链（Chen等人，2022; Gao等人，2023）和工具集成推理（Gou等人，2023）数据对DeepSeekMath-Base进行数学指令微调。得到的模型DeepSeekMath-Instruct 7B击败了所有7B规模的同类模型，并且与70B规模的开源指令微调模型相当。

此外，我们引入了群组相对策略优化（GRPO），这是近端策略优化（PPO）（Schulman等人，2017）的一个变体强化学习（RL）算法。GRPO摒弃了评论家模型，转而从群组分数中估计基线，显著减少了训练资源。仅使用英语指令微调数据的一个子集，GRPO就在强化学习阶段对强大的DeepSeekMath-Instruct模型取得了实质性改进，包括域内（GSM8K: $82.9\% \rightarrow 88.2\%$, MATH: $46.8\% \rightarrow 51.7\%$）和域外数学任务（例如，CMATH: $84.6\% \rightarrow 88.8\%$）。我们还提供了一个统一范式来理解不同的方法，如拒绝采样微调（RFT）（Yuan等人，2023a）、直接偏好优化（DPO）（Rafailov等人，2023）、PPO和GRPO。基于这样一个统一范式，我们发现所有这些方法都可以概念化为直接或简化的RL技术。我们还进行了广泛的实验，例如，在线 vs. 离线训练，结果 vs. 过程监督，单轮 vs. 迭代RL等，以深入探究这一范式的基本要素。最后，我们解释了为什么我们的RL能提升指令微调模型的性能，并基于此统一范式进一步总结了实现更有效RL的潜在方向。

### 贡献

我们的贡献包括可扩展的数学预训练，以及对强化学习的探索和分析。

**大规模数学预训练**
*   我们的研究提供了令人信服的证据，证明公开可访问的Common Crawl数据包含对数学目的有价值的信息。通过实施精心设计的数据选择流程，我们成功构建了DeepSeekMath语料库，这是一个从网页中过滤出的数学内容的高质量数据集，包含120B token，其规模几乎是Minerva（Lewkowycz等人，2022a）使用的数学网页的7倍，也是最近发布的OpenWebMath（Paster等人，2023）的9倍。
*   我们的预训练基础模型DeepSeekMath-Base 7B实现了与Minerva 540B（Lewkowycz等人，2022a）相当的性能，表明参数数量并非数学推理能力的唯一关键因素。一个在高质量数据上预训练的小模型同样可以实现强大的性能。
*   我们分享了数学训练实验的发现。在数学训练之前进行代码训练，可以提高模型使用和不使用工具解决数学问题的能力。这为长期以来的问题提供了一个部分答案：*代码训练是否提高推理能力？* 我们相信是的，至少对于数学推理而言。
*   虽然在arXiv论文上进行训练很常见，尤其是在许多数学相关的论文中，但这对本文采用的所有数学基准测试均未带来显著改善。

**强化学习的探索与分析**
*   我们引入了群组相对策略优化（GRPO），一种高效且有效的强化学习算法。GRPO摒弃了评论家模型，转而从群组分数中估计基线，与近端策略优化（PPO）相比，显著减少了训练资源。
*   我们证明了GRPO仅使用指令微调数据就显著提升了我们的指令微调模型DeepSeekMath-Instruct的性能。此外，我们观察到在强化学习过程中域外性能也得到了提升。
*   我们提供了一个统一范式来理解不同的方法，如RFT、DPO、PPO和GRPO。我们还进行了广泛的实验，例如，在线 vs. 离线训练，结果 vs. 过程监督，单轮 vs. 迭代强化学习等，以深入探究这一范式的基本要素。
*   基于我们的统一范式，我们探索了强化学习有效性的原因，并总结了几条实现LLMs更有效强化学习的潜在方向。

### 评估与度量总结

*   **英文和中文数学推理**：我们在英文和中文基准上对我们的模型进行了全面评估，涵盖了从小学到大学水平的数学问题。英文基准包括GSM8K（Cobbe等人，2021）、MATH（Hendrycks等人，2021）、SAT（Azerbayev等人，2023）、OCW Courses（Lewkowycz等人，2022a）、MMLU-STEM（Hendrycks等人，2020）。中文基准包括MGSM-zh（Shi等人，2023）、CMATH（Wei等人，2023）、高考数学填空题（Zhong等人，2023）和高考数学问答题（Zhong等人，2023）。我们评估了模型在不使用工具的情况下生成自包含文本解答的能力，以及使用Python解决问题的能力。

在英文基准上，DeepSeekMath-Base与闭源的Minerva 540B（Lewkowycz等人，2022a）相比具有竞争力，并且超越了所有开源基础模型（例如，Mistral 7B（Jiang等人，2023）和Llemma-34B（Azerbayev等人，2023）），无论它们是否经过数学预训练，并且优势通常非常明显。值得注意的是，DeepSeekMath-Base在中文基准上表现更优，这可能是因为我们没有遵循先前工作（Azerbayev等人，2023; Lewkowycz等人，2022a）来收集仅英文的数学预训练数据，而是包含了高质量的非英文数据。经过数学指令微调和强化学习后，得到的DeepSeekMath-Instruct和DeepSeekMath-RL表现出强大的性能，首次在开源社区内实现了在竞赛级MATH数据集上超过50%的准确率。

*   **形式化数学**：我们使用来自（Jiang等人，2022）的非形式化到形式化的定理证明任务在miniF2F（Zheng等人，2021）上评估DeepSeekMath-Base，选择Isabelle（Wenzel等人，2008）作为证明辅助工具。DeepSeekMath-Base展现了强大的少样本自动形式化性能。

*   **自然语言理解、推理和代码**：为了全面评估模型的一般理解、推理和编码能力，我们在Massive Multitask Language Understanding（MMLU）基准（Hendrycks等人，2020）上评估DeepSeekMath-Base，该基准包含涵盖不同学科的57个多项选择题任务；在BIG-Bench Hard（BBH）（Suzgun等人，2022）上评估，它包含23个挑战性任务，大多需要多步推理来解决；以及在HumanEval（Chen等人，2021）和MBPP（Austin等人，2021）上评估，这些是广泛用于评估代码语言模型的基准。数学预训练有益于语言理解和推理性能。

## 2 数学预训练

### 数据收集与净化

在本节中，我们将概述从Common Crawl构建DeepSeekMath语料库的过程。如图2所示，我们展示了一个迭代流程，演示了如何从Common Crawl中系统性地收集大规模数学语料库，从一个种子语料库开始（例如，一个规模小但高质量的数学相关数据集集合）。值得注意的是，这种方法也适用于其他领域，例如编码。

首先，我们选择OpenWebMath（Paster等人，2023），一个高质量数学网络文本的集合，作为我们的初始种子语料库。使用这个语料库，我们训练一个fastText模型（Joulin等人，2016）来召回更多类似OpenWebMath的数学网页。具体来说，我们从种子语料库中随机选择500，000个数据点作为正训练样本，另外从Common Crawl中选择500，000个网页作为负样本。我们使用开源库1进行训练，配置向量维度为256，学习率为0.1，单词n-gram的最大长度为3，单词出现的最小次数为3，训练轮数为3。为了减少原始Common Crawl的大小，我们采用了基于URL的去重和近似去重技术，得到了40B个HTML网页。然后，我们使用fastText模型从去重后的Common Crawl中召回数学网页。为了过滤掉低质量的数学内容，我们根据fastText模型预测的分数对收集的页面进行排序，只保留排名靠前的。保留的数据量通过对前40B、80B、120B和160B token进行预训练实验来评估。在第一次迭代中，我们选择保留前40B token。

在第一次数据收集迭代之后，仍有大量数学网页未被收集，主要是因为fastText模型是在一组缺乏足够多样性的正样本上训练的。因此，我们识别额外的数学网络来源来丰富种子语料库，以便优化fastText模型。具体来说，我们首先将整个Common Crawl组织成不相交的域；域定义为共享相同基础URL的网页集合。对于每个域，我们计算在第一次迭代中被收集的网页的百分比。超过 $10\%$ 的网页被收集的域被归类为数学相关域（例如，mathoverflow.net）。随后，我们手动标注这些已识别域内与数学内容相关的URL（例如，mathoverflow.net/questions）。链接到这些URL但未被收集的网页将被添加到种子语料库中。这种方法使我们能够收集更多正样本，从而训练一个改进的fastText模型，使其能够在后续迭代中召回更多的数学数据。经过四次数据收集迭代后，我们最终得到35.5M个数学网页，总计120B token。在第四次迭代中，我们注意到近98%的数据在第三次迭代中已被收集，因此我们决定停止数据收集。

为了避免基准污染，我们遵循Guo等人（2024）的方法，过滤掉包含来自英文数学基准（如GSM8K（Cobbe等人，2021）和MATH（Hendrycks等人，2021））以及中文基准（如CMATH（Wei等人，2023）和AGIEval（Zhong等人，2023））的问题或答案的网页。过滤标准如下：任何包含与评估基准中任何子字符串完全匹配的10-gram字符串的文本段都将从我们的数学训练语料库中移除。对于长度短于10个字符但至少3个字符的基准文本，我们使用精确匹配来过滤掉受污染的网页。

图2：一个从Common Crawl收集数学网页的迭代流程。

### 验证DeepSeekMath语料库的质量

我们进行预训练实验，以探究DeepSeekMath语料库与最近发布的数学训练语料库的对比：

*   **MathPile**（Wang等人，2023c）：一个多来源语料库（8.9B token），汇集自教科书、维基百科、ProofWiki、CommonCrawl、StackExchange和arXiv，其中大部分（超过85%）来自arXiv；
*   **OpenWebMath**（Paster等人，2023）：为数学内容过滤的CommonCrawl数据，总计13.6B token；
*   **Proof-Pile-2**（Azerbayev等人，2023）：一个数学语料库，包含OpenWebMath、AlgebraicStack（10.3B数学代码token）和arXiv论文（28.0B token）。在Proof-Pile-2上实验时，我们遵循Azerbayev等人（2023）的方法，使用arXiv:Web:Code的比例为 $2:4:1$。

#### 训练设置

我们将数学训练应用于一个具有1.3B参数的通用预训练语言模型，该模型与DeepSeek LLMs（DeepSeek-AI，2024）共享相同的框架，记作DeepSeekLLM 1.3B。我们在每个数学语料库上分别训练模型150B token。所有实验均使用高效轻量的HAI-LLM（High-flyer，2023）训练框架进行。遵循DeepSeek LLMs的训练实践，我们使用AdamW优化器（Loshchilov和Hutter，2017），其中 $\beta_{1}=0.9,\beta_{2}=0.95$，权重衰减 $=0.1$，以及一个多步学习率调度，学习率在2，000步预热后达到峰值，在训练过程的 $80\%$ 后降至其 $31.6\%$，在训练过程的 $90\%$ 后进一步降至峰值的 $10.0\%$。我们将学习率的最大值设置为 $5.3\times10^{-4}$，并使用批大小4M token，上下文长度4K。

#### 评估结果

**DeepSeekMath语料库质量高，涵盖多语言数学内容，且规模最大。**

*   **高质量**：我们使用少样本思维链提示（Wei等人，2022）在8个数学基准上评估下游性能。如表1所示，在DeepSeekMath语料库上训练的模型具有明显的性能领先优势。图3显示，在DeepSeekMath语料库上训练的模型在50B token时（Proof-Pile-2的一个完整周期）表现出比Proof-Pile-2更好的性能，表明DeepSeekMath语料库的平均质量更高。

*   **多语言**：DeepSeekMath语料库包含多种语言的数据，主要以英语和中文为代表。如表1所示，在DeepSeekMath语料库上进行训练增强了英文和中文的数学推理性能。相比之下，现有的以英文为中心的数学语料库对中文数学推理的提升有限，甚至可能阻碍其性能。

*   **大规模**：DeepSeekMath语料库比现有的数学语料库大几倍。如图3所示，当在DeepSeekMath语料库上训练时，DeepSeek-LLM 1.3B显示出更陡峭的学习曲线和更持久的改进。相比之下，基线语料库小得多，在训练过程中已经重复了多轮，导致模型性能很快达到平台期。

表1：在不同数学语料库上训练的DeepSeek-LLM 1.3B的性能，使用少样本思维链提示进行评估。语料库大小使用我们的分词器（词汇量100K）计算。

图3：在不同数学语料库上训练的DeepSeek-LLM 1.3B的基准曲线。

### 训练和评估DeepSeekMath-Base 7B

在本节中，我们将介绍DeepSeekMath-Base 7B，这是一个具有强大推理能力的基础模型，尤其是在数学方面。我们的模型使用DeepSeek-Coder-Base-v1.5 7B（Guo等人，2024）初始化，并训练了 $500$B token。数据分布如下：56%来自DeepSeekMath语料库，4%来自AlgebraicStack，10%来自arXiv，20%来自Github代码，其余10%是来自Common Crawl的英文和中文自然语言数据。我们主要采用第2.2.1节中指定的训练设置，只是将学习率的最大值设为 $4.2\times10^{-4}$，并使用批大小10M token。

我们对DeepSeekMath-Base 7B的数学能力进行了全面评估，重点关注其在不依赖外部工具的情况下生成自包含数学解决方案的能力、使用工具解决数学问题的能力以及进行形式化定理证明的能力。除了数学之外，我们还提供了基础模型更通用的概况，包括其在自然语言理解、推理和编程技能方面的表现。

**使用逐步推理解决数学问题**
我们使用少样本思维链提示（Wei等人，2022）评估DeepSeekMath-Base在解决数学问题方面的性能，涵盖8个英文和中文基准。这些基准包括定量推理（例如，GSM8K（Cobbe等人，2021）、MATH（Hendrycks等人，2021）和CMATH（Wei等人，2023））和多项选择题（例如，MMLU-STEM（Hendrycks等人，2020）和高考数学问答题（Zhong等人，2023）），涵盖了从初级到大学水平复杂性的不同数学领域。

如表2所示，在开源基础模型中，DeepSeekMath-Base 7B在所有八个基准上都处于领先地位（包括广泛使用的通用模型Mistral 7B（Jiang等人，2023）和最近发布的经过Proof-Pile-2（Azerbayev等人，2023）数学训练的Llemma 34B（Azerbayev等人，2023））。值得注意的是，在竞赛级MATH数据集上，DeepSeekMath-Base超越了现有的开源基础模型超过 $10\%$ 的绝对优势，并且优于闭源基础模型Minerva 540B（Lewkowycz等人，2022a），该模型基于PaLM（Lewkowycz等人，2022b）构建，并进一步在数学文本上训练，规模是DeepSeekMath-Base的77倍。

表2：DeepSeekMath-Base 7B与强大基础模型在英文和中文数学基准上的比较。模型使用思维链提示进行评估。Minerva结果引自Lewkowycz等人（2022a）。

**使用工具解决数学问题**
我们使用少样本程序链提示（Chen等人，2022; Gao等人，2023）在GSM8K和MATH上评估程序辅助的数学推理。模型被提示通过编写Python程序来解决每个问题，其中可以利用如 `math` 和 `sympy` 等库进行复杂计算。程序的执行结果被评估为答案。如表3所示，DeepSeekMath-Base 7B优于先前最先进的Llemma 34B。

**形式化数学**
形式化证明自动化有助于确保数学证明的准确性和可靠性，并提高效率，近年来受到越来越多的关注。我们在来自（Jiang等人，2022）的非形式化到形式化证明任务上评估DeepSeekMath-Base 7B，该任务是基于非形式化陈述、陈述的形式化对应部分以及非形式化证明生成形式化证明。我们在miniF2F（Zheng等人，2021）上评估，这是一个用于形式化奥林匹克级别数学的基准，并使用少样本提示为每个问题在Isabelle中生成形式化证明。遵循Jiang等人（2022）的方法，我们利用模型生成证明草图，并执行现成的自动化证明器Sledgehammer（Paulson，2010）来填补缺失的细节。如表3所示，DeepSeekMath-Base 7B在证明自动形式化方面表现出强大的性能。

表3：基础模型使用工具解决数学问题以及在Isabelle中进行非形式化到形式化定理证明能力的少样本评估。

**自然语言理解、推理和代码**
我们在MMLU（Hendrycks等人，2020）上评估模型自然语言理解的性能，在BBH（Suzgun等人，2022）上评估推理性能，以及在HumanEval（Chen等人，2021）和MBPP（Austin等人，2021）上评估编码能力。如表4所示，DeepSeekMath-Base 7B在其前身DeepSeek-Coder-Base-v1.5（Guo等人，2024）的基础上，在MMLU和BBH上表现出显著的性能提升，说明了数学训练对语言理解和推理的积极影响。此外，通过包含代码token进行持续训练，DeepSeekMath-Base 7B有效保持了DeepSeek-Coder-Base-v1.5在两个编码基准上的性能。总体而言，DeepSeekMath-Base 7B在三个推理和编码基准上显著优于通用模型Mistral 7B（Jiang等人，2023）。

表4：自然语言理解、推理和代码基准评估。DeepSeek-Coder-Base-v1.5†是学习率衰减前的检查点，用于训练DeepSeekMath-Base。在MMLU和BBH上，我们使用少样本思维链提示。在HumanEval和MBPP上，我们分别在零样本和少样本设置下评估模型性能。

## 3 监督微调

### 3.1 SFT数据整理

我们构建了一个数学指令微调数据集，涵盖来自不同数学领域和不同复杂度的中英文问题：问题与思维链（CoT）（Wei等人，2022）、程序链（PoT）（Chen等人，2022; Gao等人，2023）和工具集成推理格式（Gou等人，2023）的解决方案配对。训练样本总数为776K。

*   **英文数学数据集**：我们用工具集成解决方案标注GSM8K和MATH问题，并采用MathInstruct（Yue等人，2023）的一个子集以及Lila-OOD（Mishra等人，2022）的训练集，其中的问题使用CoT或PoT解决。我们的英文集合涵盖了不同的数学领域，例如代数、概率、数论、微积分和几何。
*   **中文数学数据集**：我们收集涵盖76个子主题（如线性方程）的中文K-12数学问题，解决方案以CoT和工具集成推理格式标注。

### 3.2 训练和评估DeepSeekMath-Instruct 7B

在本节中，我们介绍DeepSeekMath-Instruct 7B，它基于DeepSeekMath-Base进行了数学指令微调。训练示例被随机连接，直到达到最大上下文长度4K token。我们以批大小256和恒定学习率 $5\times10^{-5}$ 训练模型500步。

我们在4个中英文定量推理基准上评估模型在不使用和使用工具情况下的数学性能。我们将我们的模型与当时的领先模型进行基准比较：

*   **闭源模型**包括：(1) GPT系列，其中GPT-4（OpenAI，2023）和GPT-4代码解释器2是最强大的；(2) Gemini Ultra和Pro（Anil等人，2023）；(3) Inflection-2（Inflection AI，2023）；(4) Grok-1 3；以及中国公司最近发布的模型，包括(5) Baichuan-3 4；(6) GLM系列（Du等人，2022）的最新GLM-4 5。这些模型是通用目的的，大多数都经过了一系列的对齐程序。
*   2 https://openai.com/blog/chatgpt-plugins#code-interpreter
*   3 https://x.ai/model-card
*   4 https://www.baichuan-ai.com
*   5 https://open.bigmodel.cn/dev/api#glm-4

*   **开源模型**包括：通用模型如(1) DeepSeek-LLM-Chat 67B（DeepSeek-AI，2024），(2) Qwen 72B（Bai等人，2023），(3) SeaLLM-v2 7B（Nguyen等人，2023），和(4) ChatGLM3 6B（ChatGLM3 Team，2023）；以及数学增强模型，包括(5) InternLM2-Math 20B 6，它基于InternLM2构建并经历了数学训练和指令微调；(6) Math-Shepherd-Mistral 7B，它将PPO训练（Schulman等人，2017）应用于Mistral 7B（Jiang等人，2023），并带有过程监督奖励模型；(7) WizardMath系列（Luo等人，2023），它使用演进指令（即一种使用AI演进指令的指令微调版本）和PPO训练来改进Mistral 7B和Llama-2 70B（Touvron等人，2023）的数学推理，训练问题主要来源于GSM8K和MATH；(8) MetaMath 70B（Yu等人，2023），它是在GSM8K和MATH的增强版本上微调的Llama-2 70B；(9) ToRA 34B（Gou等人，2023），它是为进行工具集成数学推理而微调的CodeLlama 34B；(10) MAmmoTH 70B（Yue等人，2023），它是在MathInstruct上指令微调的Llama-2 70B。
*   6 https://github.com/InternLM/InternLM-Math

如表5所示，在禁用工具使用的评估设置下，DeepSeekMath-Instruct 7B展示了强大的逐步推理性能。值得注意的是，在竞赛级MATH数据集上，我们的模型超越了所有开源模型和大多数专有模型（例如，Inflection-2和Gemini Pro），至少领先 $9\%$ 的绝对优势。即使对于规模大得多的模型（例如，Qwen 72B）或通过专注于数学的强化学习专门增强的模型（例如，WizardMath-v1.1 7B），这一点也成立。虽然DeepSeekMath-Instruct在MATH上与中文专有模型GLM-4和Baichuan-3不相上下，但仍然逊色于GPT-4和Gemini Ultra。

在允许模型集成自然语言推理和基于程序的工具使用来解决问题的评估设置下，DeepSeekMath-Instruct 7B在MATH上接近 $60\%$ 的准确率，超越了所有现有的开源模型。在其他基准上，我们的模型与之前最先进的、规模大10倍的DeepSeek-LLM-Chat 67B相当。

表5：使用思维链和工具集成推理的开放和闭源模型在英文和中文基准上的性能。灰色分数表示使用32个候选进行多数投票的分数；其他为Top1分数。DeepSeekMath-RL 7B击败了从7B到70B的所有开源模型，以及大多数闭源模型。尽管DeepSeekMath-RL 7B仅在GSM8K和MATH的思维链格式指令微调数据上进行了进一步训练，但它在所有基准上都优于DeepSeekMath-Instruct 7B。

## 4 强化学习

### 4.1 群组相对策略优化

强化学习（RL）已被证明在监督微调（SFT）阶段后能进一步提高LLMs的数学推理能力（Luo等人，2023; Wang等人，2023a）。在本节中，我们介绍我们高效且有效的RL算法——群组相对策略优化（GRPO）。

#### 4.1.1 从PPO到GRPO

近端策略优化（PPO）（Schulman等人，2017）是一种执行者-评论家RL算法，广泛用于LLMs的RL微调阶段（Ouyang等人，2022）。具体来说，它通过最大化以下替代目标来优化LLMs：

$$\mathcal{J}_{\textit{PPO}}(\theta)=\mathbb{E}\left[q\sim P(Q),o\sim \pi_{\theta_{old}}(O|q)\right]\frac{1}{|o|}\sum_{t=1}^{|o|}\min\left[\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{old}}(o_{t}|q,o_{<t})}A_{t},\textrm{clip}\left(\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{old}}(o_{t}|q,o_{<t})},1-\epsilon,1+\epsilon\right)A_{t}\right],$$

其中 $\pi_{\theta}$ 和 $\pi_{\theta_{old}}$ 是当前和旧策略模型，$q,o$ 分别是来自问题数据集和旧策略 $\pi_{\theta_{old}}$ 抽样的问题和输出。$\epsilon$ 是PPO中引入的用于稳定训练的裁剪相关超参数。$A_{t}$ 是优势，通过基于奖励 $\{r_{\geq t}\}$ 和学习到的价值函数 $V_{\psi}$ 应用广义优势估计（GAE）（Schulman等人，2015）来计算。因此，在PPO中，需要与策略模型同时训练一个价值函数；为了缓解奖励模型的过度优化，标准方法是在每个token的奖励中添加来自参考模型的每个token的KL惩罚（Ouyang等人，2022），即：


$$r_{t}=r_{\varphi}(q,o_{\leq t})-\beta\log\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{ref}(o_{t}|q,o_{<t})},$$


其中 $r_{\varphi}$ 是奖励模型，$\pi_{ref}$ 是参考模型（通常是初始SFT模型），$\beta$ 是KL惩罚系数。

由于PPO中使用的价值函数通常是另一个与策略模型规模相当的模型，这带来了巨大的内存和计算负担。此外，在RL训练期间，价值函数在优势计算中被视为基线以减少方差。而在LLM上下文中，通常只有最后一个token被奖励模型分配奖励分数，这可能会使在每个token都准确的价值函数的训练复杂化。为了解决这个问题，如图4所示，我们提出了群组相对策略优化（GRPO），它消除了像PPO那样对额外价值函数近似的需求，转而使用对同一问题生成的多个抽样输出的平均奖励作为基线。更具体地说，对于每个问题 $q$，GRPO从旧策略 $\pi_{\theta_{old}}$ 中抽样一组输出 $\{o_{1},o_{2},\cdots,o_{G}\}$，然后通过最大化以下目标来优化策略模型：

$$
\begin{split}
\mathcal{J}_{GRPO}(\theta)&=\mathbb{E}[q\sim p(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{old}}(O|q)]\\
&\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_{i}|}\sum_{t=1}^{|o_{i}|}\left\{\min\left[\frac{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]-\beta\text{D}_{\text{KL}}\left[\pi_{\theta}||\pi_{ref}\right]\right\},
\end{split}
$$


其中 $\epsilon$ 和 $\beta$ 是超参数，$\hat{A}_{i,t}$ 是基于每个组内输出的相对奖励计算的优势，将在以下小节中详细说明。GRPO用于计算优势的群组相对方式，与奖励模型的比较性质非常契合，因为奖励模型通常是在同一问题的输出比较数据集上训练的。还请注意，GRPO不是将KL惩罚添加到奖励中，而是通过直接将训练策略与参考策略之间的KL散度添加到损失中进行正则化，避免使 $\hat{A}_{i,t}$ 的计算复杂化。

图4：PPO和我们的GRPO的演示。GRPO摒弃了价值模型，转而从群组分数中估计基线，显著减少了训练资源。

并且与（2）中使用的KL惩罚项不同，我们使用以下无偏估计量（Schulman，2020）来估计KL散度：

$$
\mathbb{D}_{KL}\left[\pi_{\theta}||\pi_{ref}\right]=\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}-\log\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}-1,
$$

保证为正。

#### 4.1.2 基于GRPO的结果监督RL

形式上，对于每个问题 $q$，从旧策略模型 $\pi_{\theta_{old}}$ 中抽样一组输出 $\{o_{1},o_{2},\cdots,o_{G}\}$。然后使用奖励模型对输出进行评分，相应地得到 $G$ 个奖励 $\mathbf{r}=\{r_{1},r_{2},\cdots,r_{G}\}$。随后，通过减去组平均值并除以组标准差来标准化这些奖励。结果监督在每个输出 $o_{i}$ 的末尾提供标准化奖励，并将输出中所有token的优势 $\hat{A}_{i,t}$ 设置为标准化奖励，即 $\hat{A}_{i,t}=\overline{r}_{i}=\frac{r_{i}-\text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}$，然后通过最大化方程（3）中定义的目标来优化策略。

#### 4.1.3 基于GRPO的过程监督RL

结果监督仅在每个输出的末尾提供奖励，这可能不足以或不高效地监督复杂数学任务中的策略。遵循Wang等人（2023b），我们也探索过程监督，它在每个推理步骤的末尾提供奖励。形式上，给定问题 $q$ 和 $G$ 个抽样输出 $\{o_{1},o_{2},\cdots,o_{G}\}$，使用过程奖励模型对输出的每个步骤进行评分，产生相应的奖励：$\mathbf{R}=\{\{r_{1}^{index(1)},\cdots,r_{1}^{index(K_{1})}\},\cdots,\{r_{G}^{index(1)},\cdots,r_{G}^{index(K_{G})}\}\}$，其中 $index(j)$ 是第 $j$ 步的结束token索引，$K_{i}$ 是第 $i$ 个输出中的总步数。我们也用平均值和标准差标准化这些奖励，即 $\overline{r}_{i}^{index(j)}=\frac{r_{i}^{index(j)}-\text{mean}(\mathbf{R})}{\text{std}(\mathbf{R})}$。随后，过程监督将每个token的优势计算为后续步骤的标准化奖励之和，即 $\hat{A}_{i,t}=\sum_{index(j)\geq t}\overline{r}_{i}^{index(j)}$，然后通过最大化方程（3）中定义的目标来优化策略。

#### 4.1.4 基于GRPO的迭代RL

随着强化学习训练过程的进行，旧的奖励模型可能不足以监督当前的策略模型。因此，我们也探索基于GRPO的迭代RL。如算法1所示，在迭代GRPO中，我们基于策略模型的抽样结果为奖励模型生成新的训练集，并使用包含 $10\%$ 历史数据的回放机制持续训练旧的奖励模型。然后，我们将参考模型设置为策略模型，并使用新的奖励模型持续训练策略模型。

### 4.2 训练和评估DeepSeekMath-RL

我们在DeepSeekMath-Instruct 7B的基础上进行RL。RL的训练数据是来自SFT数据的与GSM8K和MATH相关的思维链格式问题，大约包含144K个问题。我们排除了其他SFT问题，以研究RL在整个RL阶段缺乏数据的基准上的影响。我们遵循（Wang等人，2023b）构建奖励模型的训练集。我们基于DeepSeekMath-Base 7B以学习率 $2\times10^{-5}$ 训练初始奖励模型。对于GRPO，我们将策略模型的学习率设为 $1\times10^{-6}$。KL系数为0.04。对于每个问题，我们抽样64个输出。最大长度设置为1024，训练批大小为1024。策略模型在每个探索阶段后仅进行单次更新。我们遵循DeepSeekMath-Instruct 7B在基准上评估DeepSeekMath-RL 7B。对于DeepSeekMath-RL 7B，使用思维链推理的GSM8K和MATH可被视为域内任务，所有其他基准可被视为域外任务。

表5展示了使用思维链和工具集成推理的开放和闭源模型在英文和中文基准上的性能。我们发现：1) DeepSeekMath-RL 7B使用思维链推理分别在GSM8K和MATH上达到了 $88.2\%$ 和 $51.7\%$ 的准确率。这一性能超越了所有7B到70B范围的开源模型，以及大多数闭源模型。2) 关键的是，DeepSeekMath-RL 7B仅从DeepSeekMath-Instruct 7B开始，在GSM8K和MATH的思维链格式指令微调数据上进行了训练。尽管其训练数据范围有限，但它在所有评估指标上都优于DeepSeekMath-Instruct 7B，展示了强化学习的有效性。

## 5 讨论

在本节中，我们将分享我们在预训练和RL实验中的发现。

### 5.1 预训练中的经验教训

我们首先分享在预训练方面的经验。除非另有说明，我们将遵循第2.2.1节中概述的训练设置。值得注意的是，本节中提到的DeepSeekMath语料库是指数据收集过程第二次迭代产生的89B token数据集。

#### 5.1.1 代码训练有益于数学推理

一个流行但未经证实的假设认为代码训练可以提高推理能力。我们试图为此提供一个部分答案，特别是在数学领域：代码训练提高了模型使用和不使用工具进行数学推理的能力。

为了研究代码训练如何影响数学推理，我们实验了以下两阶段训练和单阶段训练设置：

**两阶段训练**
*   **代码训练400B Token → 数学训练150B Token**：我们训练DeepSeek-LLM 1.3B 400B代码token，然后训练150B数学token；
*   **通用训练400B Token → 数学训练150B Token**：作为对照实验，我们也尝试在第一阶段训练中使用通用token（采样自深度求索人工智能创建的大规模通用语料库）代替代码token，以研究代码token相对于通用token在提高数学推理方面的优势。

**单阶段训练**
*   **数学训练150B Token**：我们训练DeepSeek-LLM 1.3B 150B数学token；
*   **在400B代码token和150B数学token的混合上训练**：代码训练后的数学训练会降低编码性能。我们研究代码token与数学token混合进行单阶段训练时，是否仍能提高数学推理，并缓解灾难性遗忘问题。

**结果**
表6和表7展示了不同训练设置下的下游性能。

代码训练有利于程序辅助的数学推理，无论是在两阶段训练还是单阶段训练设置下。如表6所示，在两阶段训练设置下，仅代码训练就已经显著提高了使用Python解决GSM8K和MATH问题的能力。第二阶段的数学训练带来了进一步的提升。有趣的是，在单阶段训练设置下，混合代码token和数学token有效缓解了由两阶段训练引起的灾难性遗忘问题，并且协同增强了编码（表7）和程序辅助数学推理（表6）。

表6：研究不同训练设置下代码如何影响数学推理。我们使用DeepSeek-LLM 1.3B进行实验，并分别通过少样本思维链提示和少样本程序链提示评估其不使用和使用工具的数学推理性能。

表7：研究代码和数学训练的不同设置如何影响模型在语言理解、推理和编码方面的性能。我们使用DeepSeek-LLM 1.3B进行实验。我们使用少样本思维链提示在MMLU和BBH上评估模型。在HumanEval和MBPP上，我们分别进行零样本和少样本评估。

**代码训练也能提高不使用工具的数学推理。** 在两阶段训练设置下，初始的代码训练阶段已经带来了中等程度的增强。它还提高了后续数学训练的的效率，最终带来最佳性能。然而，将代码token和数学token混合进行单阶段训练会损害不使用工具的数学推理。一种推测是，DeepSeek-LLM 1.3B由于其规模有限，缺乏同时充分吸收代码和数学数据的能力。

#### 5.1.2 arXiv论文似乎对提高数学推理无效

arXiv论文通常被包含在数学预训练数据中（Azerbayev等人，2023；Lewkowycz等人，2022a；Polu和Sutskever，2020；Wang等人，2023c）。然而，关于它们对数学推理影响的详细分析尚未广泛开展。可能有些违反直觉的是，根据我们的实验，arXiv论文似乎对提高数学推理无效。我们在不同规模的模型上进行了实验，包括DeepSeek-LLM 1.3B和DeepSeek-Coder-Base-v1.5 7B（Guo等人，2024），使用了经过不同处理流程的arXiv语料库：
*   **MathPile**（Wang等人，2023c）：一个8.9B token的语料库，使用清理和过滤启发式规则开发，其中超过 $85\%$ 是科学arXiv论文；
*   **ArXiv-RedPajama**（Computer，2023）：经过移除前言、注释、宏和参考文献的arXiv LaTeX文件全集，总计28.0B token。

在我们的实验中，我们分别在每个arXiv语料库上训练DeepSeek-LLM 1.3B 150B token和DeepSeek-Coder-Base-v1.5 7B 40B token。似乎arXiv论文对提高数学推理无效。当在纯arXiv语料库上训练时，两个模型在本研究采用的各种不同复杂度的数学基准上均未显示出显著改善，甚至性能下降。这些基准包括像GSM8K和MATH这样的定量推理数据集（表8）、像MMLU-STEM这样的多项选择挑战（表8），以及像miniF2F这样的形式化数学任务（表9）。

然而，这个结论有其局限性，应谨慎对待。我们尚未研究：
*   arXiv token对本研究未包含的特定数学相关任务的影响，例如定理的非形式化（即将形式化陈述或证明转换为其非形式化版本）；
*   arXiv token与其他类型数据结合时的效果；
*   arXiv论文的好处是否会在更大的模型规模上显现。
因此，需要进一步探索，我们将其留待未来研究。

表8：在不同arXiv数据集上进行数学训练的效果。使用少样本思维链提示评估模型性能。
表9：在不同arXiv语料库上进行数学训练的效果，基础模型为DeepSeek-Coder-Base-v1.5 7B。我们在Isabelle中评估非形式化到形式化证明。

### 5.2 强化学习的见解

#### 5.2.1 迈向统一范式

在本节中，我们提供了一个统一范式来分析不同的训练方法，如SFT、RFT、DPO、PPO、GRPO，并进一步进行实验来探索该范式的要素。通常，训练方法相对于参数 $\theta$ 的梯度可以写成：

$$
\nabla_{\theta}\mathcal{J}_{\mathcal{A}}(\theta)=\mathbb{E}[(\underbrace{q,o}_{Data\ Source})\sim\mathcal{D}]\left(\frac{1}{|o|}\sum_{t=1}^{|o|}\underbrace{GC_{\mathcal{A}}(q,o,t,\pi_{rf})}_{Gradient\ Coefficient}\nabla_{\theta}\log\pi_{\theta}(o_{t}|q,o_{<t})\right).
$$

存在三个关键组成部分：1) **数据源** $\mathcal{D}$，决定训练数据；2) **奖励函数** $\pi_{rf}$，训练奖励信号的来源；3) **算法** $\mathcal{A}$：处理训练数据和奖励信号，生成决定数据惩罚或强化幅度的梯度系数 $GC$。我们基于这种统一范式分析了几种代表性方法：
*   **监督微调（SFT）**：SFT在人类选择的SFT数据上微调预训练模型。
*   **拒绝采样微调（RFT）**：RFT基于SFT问题，在从SFT模型采样的过滤输出上进一步微调SFT模型。RFT根据其答案的正确性过滤输出。
*   **直接偏好优化（DPO）**：DPO通过使用成对DPO损失在从SFT模型采样的增强输出上微调SFT模型，从而进一步优化它。
*   **在线拒绝采样微调（Online RFT）**：与RFT不同，Online RFT使用SFT模型初始化策略模型，并通过在从实时策略模型采样的增强输出上进行微调来优化它。
*   **PPO/GRPO**：PPO/GRPO使用SFT模型初始化策略模型，并用从实时策略模型采样的输出进行强化。

我们在表10中总结了这些方法的组成部分。更详细的推导过程请参见附录A.1。

**关于数据源的观察**
我们将数据源分为两类：在线采样和离线采样。在线采样表示训练数据来自实时训练策略模型的探索结果，而离线采样表示训练数据来自初始SFT模型的采样结果。RFT和DPO遵循离线风格，而Online RFT和GRPO遵循在线风格。

如图5所示，我们发现Online RFT在两个基准上显著优于RFT。具体来说，Online RFT在训练早期阶段与RFT相当，但在后期阶段获得绝对优势，展示了在线训练的优越性。这是直观的，因为在初始阶段，执行者模型与SFT模型非常相似，采样数据仅显示出微小差异。然而，在后期阶段，从执行者模型采样的数据将表现出更显著的差异，实时数据采样将提供更大的优势。

**关于梯度系数的观察**
算法将奖励信号处理为梯度系数来更新模型参数。在我们的实验中，我们将奖励函数分为“规则”和“模型”。规则指基于答案的正确性来判断响应的质量，模型指我们训练一个奖励模型来为每个响应评分。奖励模型的训练数据基于规则判断。公式10和21突出了GRPO和Online RFT之间的一个关键区别：GRPO根据奖励模型提供的奖励值独特地调整其梯度系数。这使得能够根据响应的不同幅度进行差异化的强化和惩罚。相比之下，Online RFT缺乏此特性；它不会惩罚不正确的响应，并且对所有具有正确答案的响应以相同的强度进行统一强化。

如图5所示，GRPO超越了在线RFT，从而凸显了改变正负梯度系数的效率。此外，GRPO+PS表现出比GRPO+OS更优的性能，表明使用细粒度的、步骤感知的梯度系数是有益的。此外，我们探索了迭代RL。在我们的实验中，我们进行了两轮迭代。如图6所示，我们注意到迭代RL显著提高了性能，尤其是在第一次迭代时。

表10：不同方法的数据源和梯度系数。$p_{sft}$ 表示监督微调数据集的分布。$\pi_{\theta_{sft}}$ 和 $\pi_{\theta}$ 分别表示在线训练过程中的监督微调模型和实时策略模型。

图5：使用不同方法进一步训练的DeepSeekMath-Instruct 1.3B模型在两个基准上的性能。
图6：使用DeepSeekMath-Instruct 7B在两个基准上进行迭代强化学习的性能。

#### 5.2.2 为什么RL有效？

在本文中，我们基于指令微调数据的一个子集进行强化学习，并在指令微调模型上实现了显著的性能提升。为了进一步解释为什么强化学习有效。我们评估了Instruct和RL模型在两个基准上的Pass@K和Maj@K准确率。如图7所示，RL提升了Maj@K的性能，但没有提升Pass@K。这些发现表明，RL通过使输出分布更加稳健来提高模型的整体性能，换句话说，**改进似乎归因于从TopK中提升正确答案，而不是基础能力的增强**。类似地，（Wang等人，2023a）发现了SFT模型中推理任务的**错位问题**，表明SFT模型的推理性能可以通过一系列偏好对齐策略（Song等人，2023；Wang等人，2023a；Yuan等人，2023b）来改进。

图7：在GSM8K和MATH上SFT和RL DeepSeekMath 7B的Maj@K和Pass@K（温度0.7）。注意到RL提升了Maj@K，但没有提升Pass@K。

#### 5.2.3 如何实现更有效的RL？

我们证明了RL在数学推理任务中效果非常好。我们还提供了一个统一范式来理解不同的代表性训练方法。在这个范式中，所有方法都被概念化为直接或简化的RL技术。如公式5所总结，存在三个关键组成部分：数据源、算法和奖励函数。我们为这三个组成部分提供了一些潜在的未来方向。

**数据源**
数据源是所有训练方法的原材料。在RL的背景下，我们特别将数据源指代为来自策略模型采样的未标记问题及其输出。在本文中，我们只使用了指令微调阶段的问题和朴素的核采样来采样输出。我们认为这是我们RL流程仅提高了Maj@K性能的一个潜在原因。未来，我们将在分布外问题提示上探索我们的RL流程，并结合**高级采样（解码）策略**，例如基于树搜索的方法（Yao等人，2023）。此外，**高效推理技术**（Kwon等人，2023；Leviathan等人，2023；Xia等人，2023，2024），决定了策略模型的探索效率，也扮演着极其重要的角色。

**算法**
算法将数据和奖励信号处理为梯度系数以更新模型参数。基于公式5，在某种程度上，所有方法现在都完全**信任**奖励信号的指导，来增加或减少某个token的条件概率。然而，不可能确保奖励信号总是可靠的，尤其是在极其复杂的任务中。例如，即使是经过训练有素的标注者仔细标注的PRM800K数据集（Lightman等人，2023），仍然包含大约 $20\%$ 的错误标注7。为此，我们将探索对噪声奖励信号具有鲁棒性的强化学习算法。我们相信这种**弱到强**（Burns等人，2023）的对齐方法将为学习算法带来根本性的改变。
*   7 https://github.com/openai/prm800k/issues/12#issuecomment-1728491852

**奖励函数**
奖励函数是训练信号的来源。在RL中，奖励函数通常是神经奖励模型。我们认为奖励模型存在三个重要的研究方向：
1.  **如何增强奖励模型的泛化能力。** 奖励模型必须能有效泛化以处理分布外问题和高级解码输出；否则，强化学习可能只是稳定了LLMs的分布，而不是提高其基本能力。
2.  **如何反映奖励模型的不确定性。** 这种不确定性可能充当弱奖励模型和弱到强学习算法之间的桥梁。
3.  **如何高效构建高质量的过程奖励模型**，以为推理过程提供细粒度的训练信号（Lightman等人，2023；Wang等人，2023b）。

## 6 结论、局限性与未来工作

我们提出了DeepSeekMath，它在竞赛级MATH基准上超越了所有开源模型，并接近闭源模型的性能。DeepSeekMath使用DeepSeek-Coder-v1.5 7B初始化，并进行了500B token的持续训练，其中训练数据的重要组成部分是来自Common Crawl的120B数学token。我们广泛的消融研究表明，网页为高质量数学数据提供了巨大潜力，而arXiv可能不如我们预期的那么有益。我们引入了群组相对策略优化（GRPO），这是近端策略优化（PPO）的一个变体，它可以显著提高数学推理能力，同时减少内存消耗。实验结果表明，即使DeepSeekMath-Instruct 7B已经在基准上达到了高分，GRPO仍然有效。我们还提供了一个统一范式来理解一系列方法，并总结了几个实现更有效强化学习的潜在方向。

尽管DeepSeekMath在定量推理基准上取得了令人印象深刻的分数，但它在几何和定理证明方面的能力相对闭源模型较弱。例如，在我们的试运行中，模型无法处理与三角形和椭圆相关的问题，这可能表明预训练和微调中存在数据选择偏差。此外，受模型规模限制，DeepSeekMath在少样本能力上比GPT-4差。GPT-4可以通过少样本输入提高其性能，而DeepSeekMath在零样本和少样本评估中表现出相似的性能。未来，我们将进一步改进我们设计的数据选择流程，以构建更高质量的预训练语料库。此外，我们将探索LLMs更有效强化学习的潜在方向（第5.2.3节）。

## 参考文献

（参考文献列表与原文保持一致，此处为节省空间略去，实际输出应包含所有参考文献条目）