# DeepSeek-Coder-V2：突破代码智能中闭源模型的壁垒

邱浩朱*、郭大亚*、邵志宏*、杨德建*、王佩艺、徐润新、吴 Y.

李宇坤、高华卓、马世荣、曾王定、毕晓、顾子辉、徐汉威、戴达迈

董凯、张立跃、朴艺时、勾志斌、谢振达、郝哲文、王炳轩

宋俊晓、陈德利、谢鑫、关康、游宇翔、刘爱心、杜秋实、高文俊

卢璇、陈钦宇、王耀辉、邓承琪、李家石、赵成刚

阮冲、罗 Full、梁文峰

DeepSeek-AI

https://github.com/deepseek-ai/DeepSeek-Coder-V2

###### 摘要

我们提出了 DeepSeek-Coder-V2，一个开源混合专家（MoE）代码语言模型，其在代码特定任务上实现了与 GPT4-Turbo 相媲美的性能。具体而言，DeepSeek-Coder-V2 是在 DeepSeek-V2 的一个中间检查点基础上，使用额外的 6 万亿 token 进行持续预训练得到的。通过这种持续预训练，DeepSeek-Coder-V2 显著增强了 DeepSeek-V2 的编码和数学推理能力，同时保持了在通用语言任务上的可比性能。与 DeepSeek-Coder-33B 相比，DeepSeek-Coder-V2 在代码相关任务的各个方面，以及推理和通用能力上都展现出了显著的进步。此外，DeepSeek-Coder-V2 将其支持的编程语言数量从 86 种扩展到 338 种，同时将上下文长度从 16K 扩展到 128K。在标准基准评估中，DeepSeek-Coder-V2 在编码和数学基准测试中，相比 GPT4-Turbo、Claude 3 Opus 和 Gemini 1.5 Pro 等闭源模型，取得了更优越的性能。


## 1 引言

开源社区通过开发 StarCoder (Li et al., 2023b; Lozhkov et al., 2024)、CodeLlama (Roziere et al., 2023)、DeepSeek-Coder (Guo et al., 2024) 和 Codestral (MistralAI, 2024) 等开源代码模型，在推进代码智能方面取得了重大进展。这些模型稳步逼近闭源模型的性能水平，为代码智能的进步做出了贡献。然而，与 GPT4-Turbo (OpenAI, 2023)、Claude 3 Opus (Anthropic, 2024) 和 Gemini 1.5 Pro (Reid et al., 2024) 等最先进的闭源模型相比，仍存在明显的差距。为了弥合这一差距并进一步推动开源代码模型的发展，我们推出了 DeepSeek-Coder-V2 系列。这些模型建立在 DeepSeek-V2 (DeepSeek-AI, 2024) 的基础上，并使用额外的包含 6 万亿 token 的语料库进行了进一步的预训练。

在预训练阶段，DeepSeek-Coder-V2 的数据集由 60% 的源代码、10% 的数学语料和 30% 的自然语言语料组成。源代码包含来自 GitHub 和 CommonCrawl 的 1,170B 代码相关 token，使用了与 DeepSeekMath (Shao et al., 2024) 相同的处理流程。与用于训练 DeepSeek-Coder 的代码语料相比，该语料库涵盖的编程语言从 86 种扩展到 338 种。为了展示新代码语料的有效性，我们使用 1B 参数模型进行了消融研究，观察到在 HumanEval (从 30.5% 提升到 37.2%) 和 MBPP (从 44.6% 提升到 54.0%) 基准测试 (Austin et al., 2021a; Chen et al., 2021) 上的准确率分别提高了 6.7% 和 9.4%。对于数学语料，我们使用相同的流程从 CommonCrawl 收集了 221B 数学相关 token，大约是 120B DeepSeekMath 语料库 (Shao et al., 2024) 的两倍；而对于自然语言语料，我们直接从 DeepSeek-V2 的训练语料中进行采样。总体而言，DeepSeek-Coder-V2 接触了 10.2T 的训练 token，其中 4.2 万亿 token 来自 DeepSeek V2 数据集，其余 6 万亿 token 来自 DeepSeek-Coder-V2 数据集。

为了适应更长的代码输入并增强在各种编程场景下的适用性，我们将上下文长度从 16K token 扩展到 128K token，使我们的模型能够处理更复杂和更广泛的编码任务。在此多来源语料库上对 DeepSeek-V2 进行持续预训练后，我们发现 DeepSeek-Coder-V2 显著增强了模型的编码和数学推理能力，同时保持了可比的通用语言性能。

在对齐阶段，我们首先构建了一个指令训练数据集，其中包含来自 DeepSeek-Coder (Guo et al., 2024) 和 DeepSeek-Math (Shao et al., 2024) 的代码和数学数据，以及来自 DeepSeek-V2 (DeepSeek-AI, 2024) 的通用指令数据。该数据集用于对基础模型进行微调。然后，在强化学习阶段，我们采用组相对策略优化（GRPO）算法将其行为与人类偏好对齐。我们使用编译器反馈和测试用例在编码领域收集偏好数据，并开发了一个奖励模型来指导策略模型的训练。这种方法确保了模型的响应在编码任务中针对正确性和人类偏好进行了优化。为了使模型在对齐后支持代码补全功能，我们还在使用 16B 参数基础模型的微调过程中采用了填充中间（Fill-In-Middle）方法 (Guo et al., 2024)。

### 贡献

总而言之，我们的主要贡献如下：

*   我们基于 DeepSeek MoE 框架，推出了具有 16B 和 236B 参数的 DeepSeek-Coder-V2，其激活参数量仅为 2.4B 和 21B，高效支持多样化的计算和应用需求。此外，DeepSeek-Coder-V2 支持 338 种编程语言和最大 128K token 的上下文长度。
*   我们首次尝试开发一个开源的上百亿参数代码模型，以推进代码智能领域的发展。实验结果表明，DeepSeek-Coder-V2 236B 在编码和数学任务上的表现均优于 GPT4-Turbo、Claude 3 Opus 和 Gemini 1.5 Pro 等最先进的闭源模型。
*   DeepSeek-Coder-V2 模型在宽松许可证下公开发布，允许用于研究和不受限制的商业用途。

### 评估与指标概览

*   **代码**：在代码生成基准评估方面，DeepSeek-Coder-V2 相比所有开源模型展现出显著优势，同时其性能与 GPT4-Turbo、Claude 3 Opus 和 Gemini 1.5 Pro 等领先闭源模型相当。值得注意的是，我们在 HumanEval (Chen et al., 2021) 上取得了 **90.2%** 的分数，在 MBPP (Austin et al., 2021a) 上取得了 **76.2%** 的分数（使用 EvalPlus 评估流程建立了新的最先进结果），在 LiveCodeBench (Jain et al., 2024) 上取得了 **43.4%** 的分数（包含 2023年12月至2024年6月的题目）。此外，DeepSeek-Coder-V2 是首个在 SWEBench (Jimenez et al., 2023) 上超过 10% 分值的开源模型。
*   **数学**：DeepSeek-Coder-V2 展现出强大的数学推理能力，在 GSM8K (Cobbe et al., 2021) 等初级基准测试以及 MATH (Hendrycks et al., 2021)、AIME (MAA, 2024) 和 Math Odyssey (Netmind AI, 2024) 等高级竞赛级基准测试中，均能与 GPT-4o、Gemini 1.5 Pro 和 Claude 3 Opus 等顶尖闭源模型相媲美。值得注意的是，DeepSeek-Coder-V2 在 MATH 基准测试中达到了 **75.7%** 的准确率，几乎接近 GPT-4o 实现的 **76.6%** 的最先进准确率。此外，它在 AIME 2024 竞赛中的表现超过了这些闭源模型。
*   **自然语言**：DeepSeek-Coder-V2 保持了与 DeepSeek-V2 可比的通用语言性能。例如，DeepSeek-Coder-V2 使用 OpenAI simple-eval 流程在 MMLU 上取得了 79.2% 的成绩。在使用 GPT-4 作为评判者的主观评估中，DeepSeek-Coder-V2 在 arena-hard (Li et al., 2024) 上得分为 **65.0**，在 MT-bench (Zheng et al., 2023) 上得分为 **8.77**，在 alignbench (Liu et al., 2023c) 上得分为 **7.84**。这些分数显著优于其他代码专用模型，甚至可以与通用开源模型相媲美。

## 2 数据收集

DeepSeek-Coder-V2 的预训练数据主要由 60% 的源代码、10% 的数学语料和 30% 的自然语言语料组成。由于自然语言语料是直接从 DeepSeek-V2 的训练数据集中采样的，本节重点介绍代码和数学数据的收集、清理和过滤过程。同时，我们通过对比分析实验进一步验证了这些数据的质量。

我们收集了 GitHub 上 2023 年 11 月之前创建的公共仓库。我们首先应用与 DeepSeek-Coder (Guo et al., 2024) 相同的过滤规则和近重复删除技术，以过滤掉低质量和重复的源代码。为了使论文内容完整，我们简要描述一下过滤规则。首先，我们过滤掉平均行长度超过 100 个字符或最大行长度超过 1000 个字符的文件。此外，我们删除了字母字符占比低于 25% 的文件。除 XSLT 编程语言外，我们还过滤掉前 100 个字符中出现字符串 `"<?xml version="` 的文件。对于 HTML 文件，我们考虑可见文本与 HTML 代码的比例。我们保留可见文本至少占代码 20% 且不少于 100 个字符的文件。对于通常包含更多数据的 JSON 和 YAML 文件，我们只保留字符数在 50 到 5000 个字符之间的文件。这有效地移除了大多数数据量大的文件。通过应用这些过滤规则和近重复删除，我们获得了涵盖 338 种编程语言的 821B 代码，以及 185B 的代码相关文本（如 Markdown 和 issues）。支持的编程语言列表见附录 A。我们使用与 DeepSeekV2 相同的分词器，详见 (DeepSeek-AI, 2024)。

为了从 Common Crawl 收集代码相关和数学相关的网页文本，我们遵循与 DeepSeekMath (Shao et al., 2024) 相同的流程。具体来说，我们选择 StackOverflow[^stackoverflow]、PyTorch 文档[^pytorch] 等库网站以及 StackExchange[^stackexchange] 等数学网站作为我们的初始种子语料库。使用这个种子语料库，我们训练了一个 fastText 模型 (Joulin et al., 2016) 来召回更多与编码和数学相关的网页。由于中文等语言的分词不能通过空格完成，我们使用了 DeepSeek-V2 的字节对编码（BPE）分词器，这显著提高了 fastText 的召回准确率。对于每个领域，我们计算第一次迭代中收集的网页百分比。收集网页超过 10% 的领域被归类为代码相关或数学相关。然后，我们标注这些已识别领域中与代码相关或数学相关内容的 URL。链接到这些 URL 的未收集网页被添加到种子语料库中。经过三轮数据收集，我们从网页中收集了 700 亿个代码相关 token 和 221B 个数学相关 token。为了进一步从 GitHub 收集高质量的源代码，我们还在 GitHub 上应用了相同的流程，经过两轮数据收集，收集了 94B 的源代码。初始种子语料库是通过手动收集高质量源代码（例如包含详细描述的代码）构建的。最后，新的代码语料库包含来自 GitHub 和 CommonCrawl 的 1,170B 个代码相关 token。

[^stackoverflow]: https://stackoverflow.com
[^pytorch]: https://pytorch.org/docs
[^stackexchange]: https://math.stackexchange.com

为了展示新代码语料的有效性，我们使用 1B 参数模型进行了消融研究（见表 1），并与用于训练 DeepSeek-Coder 的语料库进行了比较。在新代码语料库上使用 1T token 对 1B 模型进行预训练，导致在 HumanEval（从 30.5% 到 36.0%）和 MBPP（从 44.6% 到 49.0%）基准测试上的准确率分别提高了 5.5% 和 4.4%。进一步使用 2T token 训练 1B 模型带来了额外的改进，HumanEval 和 MBPP 分数分别上升到 37.2% 和 54.0%。因此，新的代码语料库优于用于训练 DeepSeek-Coder 的代码语料库。

表 1：DeepSeek-Coder 与 DeepSeek-Coder-V2 的 1B 基础模型性能比较。

| 模型 | Token 数量 | Python | C++ | Java | PHP | TS | C# | Bash | JS | Avg | MBPP |
|---|---|---|---|---|---|---|---|---|---|---|---|
| DeepSeek-Coder-1B | 1T | 30.5% | 28.0% | 31.7% | 23.0% | 30.8% | 31.7% | 9.5% | 28.6% | 26.7% | 44.6% |
| DeepSeek-Coder-V2-1B | 1T | 36.0% | 34.8% | 31.7% | 27.3% | **37.7**% | 34.2% | 6.3% | **38.5**% | 31.2% | 49.0% |
| DeepSeek-Coder-V2-1B | 2T | **37.2**% | **39.1**% | **32.3**% | **31.7**% | 34.6% | **36.7**% | **12.0**% | 32.9% | **32.0**% | **54.0**% |

## 3 训练策略

### 3.1 训练目标

我们对 DeepSeek-Coder-v2 16B 使用两种训练目标：下一个 Token 预测（Next-Token-Prediction）和填充中间（Fill-In-Middle, FIM）(Bavarian et al., 2022; Guo et al., 2024; Li et al., 2023b)。对于 DeepSeek-Coder-v2 236B，我们只使用下一个 Token 预测目标。这里简要介绍一下 FIM 训练策略。我们采用 FIM 训练方法来开发 DeepSeek-Coder-v2-16B，利用 PSM（前缀、后缀、中间）模式。该方法按照以下顺序重构内容：前缀、后缀、中间，如下所示：

$$<|\texttt{fim\_begin}|>_{fpre}<|\texttt{fim\_hole}|>_{fsuf}<|\texttt{fim\_end}|>_{fmiddle}<|\texttt{eos\_token}|>$$

此结构在文档级别应用，作为预打包过程的一部分。FIM 的使用率为 0.5，与 PSM 框架一致，以增强训练效果和模型性能。

### 3.2 模型架构

我们的架构与 DeepSeekV2 (DeepSeek-AI, 2024) 保持一致。超参数设置（16B 和 236B）分别对应于 DeepSeek-V2-Lite 和 DeepSeek-V2 中使用的设置。值得注意的是，我们在训练过程中遇到了不稳定和梯度值尖峰的问题，我们将其归因于指数归一化技术。为了解决这个问题，我们恢复使用了传统的归一化方法。

### 3.3 训练超参数

与 DeepSeek V2 方法 (DeepSeek-AI, 2024) 一致，我们使用 AdamW 优化器 (Loshchilov and Hutter, 2019)，配置为 $\beta_{1}=0.9$，$\beta_{2}=0.95$，权重衰减为 0.1。批大小和学习率根据 DeepSeek-V2 的规范进行调整。对于学习率调度，我们采用余弦衰减策略，以 2000 个预热步开始，逐渐将学习率降低到其初始值的 $10\%$。

DeepSeek-Coder-V2 和 DeepSeek-Coder-V2-Lite 都使用相同的方法进行训练。为了在 DeepSeek-Coder-V2 中保持强大的自然语言理解能力，我们从 DeepSeek-V2 的一个中间检查点继续预训练过程。该中间检查点最初在 4.2T token 上进行了训练。因此，DeepSeek-Coder-V2 在预训练阶段总共接触了 10.2T 高质量 token。

表 2：DeepSeek-Coder-V2 的训练设置。

| 模型 | DeepSeek-Coder-V2-Lite | DeepSeek-Coder-V2 |
|---|---|---|
| # 总参数 (#TP) | 16B | 236B |
| # 激活参数 (#AP) | 2.4B | 21B |
| 预训练 Token 数 | 4.2T+6T | 4.2T+6T |
| LR 调度器 | Cosine | Cosine |
| FIM | 启用 | 禁用 |

### 3.4 长上下文扩展

遵循 DeepSeek-V2，我们使用 Yarn (Peng et al., 2023) 将 DeepSeek-Coder-V2 的上下文长度扩展到 128K。YARN 的超参数与 DeepSeek-V2 相同：缩放因子 $s$ 为 40，$\alpha$ 为 1，$\beta$ 为 32。我们进一步分两个阶段继续训练模型，以增强其处理长上下文的能力。在第一阶段，我们使用 32K 的序列长度和 1152 的批大小训练 1000 步。在第二阶段，我们使用 128K 的序列长度和 288 个序列的批大小额外训练 1000 步。

需要指出的是，在长上下文扩展期间，我们上调了长上下文数据的比例。如图 2 所示，“Needle In A Haystack”（NIAH）测试的结果表明，DeepSeek-Coder-V2 在高达 128K 的所有上下文窗口长度上都表现良好。

### 3.5 对齐

#### 3.5.1 监督微调

为了构建 DeepSeek-Coder-V2 Chat，我们构建了混合代码和数学数据的指令训练数据集。我们首先从 DeepSeek-Coder 和 DeepSeek-Math 收集了 20k 条代码相关指令数据和 30k 条数学相关数据。为了保持通用能力，我们还从 DeepSeek-V2 的指令数据中采样了一些数据。最后，我们使用了一个包含 300M token 的指令数据集进行训练。训练时，我们使用余弦调度，预热步数为 100，初始学习率为 $5e^{-6}$。我们还使用了 1M token 的批大小，总共 1B token。

#### 3.5.2 强化学习

我们进一步采用强化学习（RL）技术来充分挖掘 DeepSeek-Coder-V2 的能力，这被证明是非常有效的。

**提示词**：我们花费了大量精力从各种来源收集与代码和数学相关的提示词，每个代码提示词都配有相应的测试用例。过滤后，总共约有 40k 条数据。

**奖励建模**：奖励模型在 RL 训练中起着至关重要的作用。对于数学偏好数据，我们使用真实标签来获取。对于代码偏好数据，虽然代码编译器本身已经可以提供 0-1 反馈（代码是否通过所有测试用例），但一些代码提示词的测试用例数量可能有限，覆盖率不高，因此直接使用编译器提供的 0-1 反馈可能有噪声且不是最优的。因此，我们仍然决定在编译器提供的数据上训练一个奖励模型，并在 RL 训练期间使用奖励模型提供信号，这比原始的编译器信号更加稳健，并具有更好的泛化能力。如图 3 所示，在我们内部的测试集（Leetcode 和 Leetcode-zh）上，使用奖励模型提供 RL 训练信号明显优于使用原始编译器信号。因此，在后续所有实验中，我们都使用奖励模型信号而非编译器信号。

**强化学习算法**：我们采用组相对策略优化（GRPO）算法 (Shao et al., 2024) 作为我们的 RL 算法，与 DeepSeek-V2 使用的算法相同。值得注意的是，GRPO 被证明非常有效，并且与 PPO 相比成本更低，因为不需要维护额外的评论家模型。

## 4 实验结果

在本节中，我们在三种类型的任务上评估 DeepSeek-Coder-V2，包括编码、数学和通用自然语言。我们将 DeepSeek-Coder-V2 与之前最先进的大语言模型进行比较。

*   **CodeLlama** (Roziere et al., 2023) 是一系列基于 Llama2 (Touvron et al., 2023) 的代码语言模型，并在 5000 亿到 10000 亿代码 token 的数据集上进行了持续预训练。这些模型有四种规模：7B、13B、34B 和 70B。
*   **StarCoder** (Lozhkov et al., 2024) 是一个具有 150 亿参数的可公开访问模型。它专门在 Stack 数据集 (Kocetkov et al., 2022) 精心策划的子集上训练，涵盖 86 种编程语言。
*   **StarCoder2** (Lozhkov et al., 2024) 包含 3B、7B 和 15B 参数模型，在 Stack2 数据集 (Lozhkov et al., 2024) 的 3.3 到 4.3 万亿 token 上训练，涵盖 619 种编程语言。
*   **DeepSeek-Coder** (Guo et al., 2024) 包含一系列代码语言模型，参数范围从 10 亿到 330 亿。每个模型都在 2 万亿 token 上从头开始训练，由 87% 的代码和 13% 的中英文自然语言组成。这些模型使用 16K 的窗口大小和额外的填空任务在项目级代码语料库上进行预训练，从而支持项目级代码补全和填充。
*   **Codestral** (MistralAI, 2024) 是由 Mistral 开发的 220 亿参数模型。它在包含 80 多种编程语言的多样化数据集上训练，包括 Python、Java 和 JavaScript 等流行语言，以及 Swift 和 Fortran 等更专业的语言。

我们比较的通用语言模型包括 **Llama3 70B** (Meta, 2024)、**GPT-4** (OpenAI, 2023)、**Claude 3 Opus** (Anthropic, 2024) 和 **Gemini 1.5 Pro** (Reid et al., 2024)。虽然它们没有专门在大型代码语料库上进行训练，但它们在编码方面达到了最先进的性能。

### 4.1 代码生成

**HumanEval 和 MBPP 基准测试**。HumanEval (Chen et al., 2021)[^he] 和 MBPP (Austin et al., 2021a) 基准测试通常用于评估代码生成大语言模型（LLMs）的性能。HumanEval 包含 164 个 Python 任务，通过测试用例验证，以评估代码 LLMs 在零样本场景下的性能。对于 MBPP，我们使用 MBPP-Plus 版本 (Liu et al., 2023a) 来评估模型。为了测试模型的多语言能力，我们将 HumanEval 基准问题扩展到另外七种语言：C++、Java、PHP、TypeScript、C#、Bash、JavaScript、Swift、R、Julia、D、Rust 和 Racket。对于这两个基准测试，我们采用了贪婪搜索策略，并使用相同的脚本和环境重新生成了基线结果，以确保公平比较。

[^he]: 我们使用模板“请完成下面的 Python 函数。你函数的最终完整版本必须在代码块中返回。以下是未完成的函数：\n ^\^ \^ python\n|问题描述|\n\n”来构建指令提示。

表 3 详细概述了各种模型在 HumanEval 和 MBPP+ 基准测试上跨多种编程语言的性能指标。DeepSeek-Coder-V2-Instruct 表现出卓越的性能，获得了第二高的平均分 75.3%。这一表现值得注意，因为它打破了闭源模型通常占据的主导地位，成为领先的开源竞争者。它仅被 GPT-4o 超越，后者的平均分为 76.4% 领先。DeepSeek-Coder-V2-Instruct 在各种语言上均表现出顶级结果，包括在 Java 和 PHP 上取得最高分，在 Python、C++、C#、TypeScript 和 JavaScript 上表现出色，凸显了其处理多样化编码挑战的稳健性和多功能性。

此外，DeepSeek-Coder-V2-Lite-Instruct 的表现也令人印象深刻，超过了更大的 33B 模型。其平均性能有相当大的优势（65.6% 对比 61.9%），突显了 16B 模型尽管规模较小，但在提供有竞争力结果方面的有效性。这强调了模型的效率以及模型架构和训练方法的进步，使其能够超越更大的对手。

表 3：各种模型在 HumanEval 和 MBPP 基准测试上的性能指标

| 模型 | #TP | #AP | Python | Java | C++ | C# | TS | JS | PHP | Bash |
|---|---|---|---|---|---|---|---|---|---|---|
| **闭源模型** | | | | | | | | | | |
| Gemini-1.5-Pro | - | - | 83.5% | 81.0% | 78.3% | 75.3% | 77.4% | 80.8% | 74.5% | 39.9% |
| Claude-3-Opus | - | - | 84.2% | 78.5% | 81.4% | 74.7% | 76.1% | 75.8% | 78.3% | 48.7% |
| GPT-4-1106 | - | - | 87.8% | **82.3**% | 78.9% | 80.4% | 81.8% | 80.1% | 77.6% | **55.7**% |
| GPT-4-Turbo-0409 | - | - | 88.2% | 81.7% | 78.3% | 79.1% | 79.3% | 80.8% | 78.9% | **55.1**% |
| GPT-4o-0513 | - | - | **91.0**% | 80.4% | **87.0**% | **82.9**% | **86.2**% | **87.6**% | **79.5**% | 53.8% |
| **开源模型** | | | | | | | | | | |
| Codestral | 22B | 22B | 78.1% | 71.5% | 71.4% | 77.2% | 72.3% | 73.9% | 69.6% | 47.5% |
| DS-Coder-instruct | 33B | 33B | 79.3% | 73.4% | 68.9% | 74.1% | 67.9% | 73.9% | 72.7% | 43.0% |
| Llama3-Instruct | 70B | 70B | 81.1% | 67.7% | 64.0% | 69.6% | 69.8% | 70.2% | 65.8% | 36.1% |
| DS-Coder-V2-Lite-Instruct | 16B | 24B | 81.1% | 76.6% | 75.8% | 76.6% | 80.5% | 77.6% | 74.5% | 43.0% |
| DS-Coder-V2-Instruct | 236B | 21B | **90.2**% | **82.3**% | **84.8**% | **82.3**% | **83.0**% | **84.5**% | **79.5**% | **52.5**% |

| 模型 | #TP | #AP | Swift | R | Julia | D | Rust | Racket | MBPP+ | Average |
|---|---|---|---|---|---|---|---|---|---|---|
| **闭源模型** | | | | | | | | | | |
| Gemini-1.5-Pro | - | - | 66.5% | 53.4% | 71.7% | 55.8% | 73.1% | 48.4% | **74.6**% | 68.9% |
| Claude-3-Opus | - | - | 63.9% | 55.9% | 76.1% | 60.3% | 71.2% | **64.6**% | 72.0% | 70.8% |
| GPT-4-1106 | - | - | 62.7% | 57.8% | 69.2% | 60.9% | **78.8**% | 64.0% | 69.3% | 72.5% |
| GPT-4-Turbo-0409 | - | - | 63.9% | 56.5% | 69.8% | **61.5**% | **78.8**% | 63.4% | 72.2% | 72.3% |
| GPT-4o-0513 | - | - | **75.9**% | **65.2**% | **78.0**% | 60.9% | **80.1**% | **64.6**% | 73.5% | **76.4**% |
| **开源模型** | | | | | | | | | | |
| Codestral | 22B | 22B | 63.3% | 49.7% | 67.9% | 32.1% | 67.3% | 37.3% | 68.2% | 63.2% |
| DS-Coder-instruct | 33B | 33B | 61.4% | 44.7% | 53.5% | 31.4% | 68.6% | 46.0% | 70.1% | 61.9% |
| Llama3-Instruct | 70B | 70B | 55.1% | 46.0% | 62.9% | 48.1% | 58.3% | 46.0% | 68.8% | 60.6% |
| DS-Coder-V2-Lite-Instruct | 16B | 24B | 64.6% | 47.8% | 67.3% | 45.5% | 62.2% | 41.6% | 68.8% | 65.6% |
| DS-Coder-V2-Instruct | 236B | 21B | **72.2**% | **64.0**% | **72.3**% | **64.1**% | **78.2**% | **63.4**% | **76.2**% | **75.3**% |

**竞技编程**。为了进一步验证模型在实际竞技编程问题上的能力，我们利用 LiveCodeBench (Jain et al., 2024) 和 USACO 基准测试 (Shi et al., 2024) 来评估 DeepSeek-Coder-V2 的有效性。LiveCodeBench 是对大语言模型代码生成的细致且无污染的评估，系统地随时间收集来自 LeetCode、AtCoder 和 CodeForces 这三个著名竞技编程平台的新颖挑战。由于训练数据的截止日期是 2023 年 11 月之前，我们使用了 Livecodebench 的一个子集（1201-0601）。USACO 基准测试包含来自美国计算机奥林匹克竞赛的 307 个问题，每个问题都配有高质量的单元测试、参考代码和官方分析。

表 4 展示了各种语言模型在两个基准测试上的性能。值得注意的是，DeepSeek-Coder-V2-Instruct 表现突出，在大模型中得分并列最高，为 43.4%，与 GPT-4o 相当。这一优异表现使其总体排名第二，仅次于总体性能领先的 GPT-4-Turbo-0409（45.7%）。DeepSeek-Coder-V2-Instruct 在处理复杂编码挑战方面的卓越能力，使其牢固地成为顶级竞争者，紧随领先的 GPT-4-Turbo 变体。

表 4：LiveCodeBench (LCB) 和 USACO 基准测试上的性能。

| 模型 | #TP | #AP | LiveCodeBench | USACO |
|---|---|---|---|---|
| | | | Easy (82) | Medium (87) | Hard (57) | Overall (226) | |
| **闭源模型** | | | | | | | |
| Gemini-1.5-Pro | - | - | 74.9% | 16.8% | 1.8% | 34.1% | 4.9% |
| Claude-3-Opus | - | - | 77.2% | 16.7% | 0.7% | 34.6% | 7.8% |
| GPT-4-1106 | - | - | 78.4% | 20.2% | 3.5% | 37.1% | 11.1% |
| GPT-4-Turbo-0409 | - | - | 84.1% | **35.4**% | **6.1**% | **45.7**% | 12.3% |
| GPT-4o-0513 | - | - | **87.4**% | 27.5% | 4.9% | 43.4% | **18.8**% |
| **开源模型** | | | | | | | |
| Codestral | 22B | 22B | 66.5% | 17.7% | 0.2% | 31.0% | 4.6% |
| DS-Coder-instruct | 33B | 33B | 51.6% | 9.7% | 0.4% | 22.5% | 4.2% |
| Llama3-Instruct | 70B | 70B | 62.4% | 14.4% | 2.1% | 28.7% | 3.3% |
| DS-Coder-V2-Lite-Instruct | 16B | 2.4B | 58.5% | 8.0% | 0.0% | 24.3% | 6.5% |
| DS-Coder-V2-Instruct | 236B | 21B | **84.1**% | **29.9**% | **5.3**% | **43.4**% | **12.1**% |

### 4.2 代码补全

#### 4.2.1 仓库级代码补全评估

我们使用 RepoBench (Liu et al., 2023b) 来评估当前可用的、大小在 35B 以下的开源代码模型在仓库级代码补全任务中的能力。该数据集是从两种流行编程语言（Python 和 Java）中多样化、真实世界、开源、宽松许可证的仓库中构建的。值得注意的是，RepoBench 的最新版本（v1.1）的数据来源于 2023 年 10 月 6 日至 12 月 31 日创建的 GitHub 仓库，而我们的预训练数据包含 2023 年 11 月之前创建的代码。为了确保该数据集不在我们的预训练数据中并避免数据泄露，我们只使用 2023 年 12 月的数据。

我们的评估包括五个上下文长度级别——2k、4k、8k、12k 和 16k token——涵盖三种设置：跨文件优先、跨文件随机和文件内。我们对所有评估模型使用贪婪搜索。模型被限制为每个提示最多生成 64 个新 token，输出的第一个非空且非注释行被选为预测。通过截断多余的跨文件上下文，提示的最大 token 长度设置为 15,800。我们报告了不同上下文长度级别的平均精确匹配率。

如表 5 所示，结果表明，尽管 DeepSeek-Coder-V2-Lite-Base 模型只有 24 亿激活参数，但其 Python 代码补全能力与 DeepSeek-Coder-Base 33B 模型相当，Java 代码补全能力与 DeepSeek-Coder-Base 7B 模型相当。与 CodeStral 相比，DeepSeek-Coder-V2-Lite-Base 模型的激活参数只有 CodeStral 的十分之一，导致在代码补全任务中性能较低。然而，我们相信，DeepSeek-Coder-V2 较少的激活参数使其在代码补全场景中速度更快。

表 5：不同模型在 RepoBench v1.1 的 12 月子集上的性能。

| 模型 | #TP | #AP | Python | | | | | | Java | | | | | |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| | | | 2k | 4k | 8k | 12k | 16k | Avg | 2k | 4k | 8k | 12k | 16k | Avg |
| StarCoder2-Base | 15B | 15B | 35.7% | 36.7% | 34.6% | 27.4% | 25.1% | 32.1% | 46.2% | 45.0% | 39.8% | 30.5% | 30.7% | 38.7% |
| CodeLlama-Base | 7B | 7B | 32.0% | 34.4% | 35.3% | 33.3% | 22.2% | 33.5% | 43.1% | 42.1% | 40.4% | 37.0% | 40.3% | 40.6% |
| CodeLlama-Base | 13B | 13B | 33.0% | 36.5% | 37.0% | 34.6% | 35.0% | 35.2% | 43.5% | 44.8% | 40.7% | 38.6% | 41.1% | 41.8% |
| CodeLlama-Base | 34B | 34B | 35.3% | 37.5% | 39.5% | 34.9% | 35.6% | 36.6% | 45.9% | 45.4% | 42.5% | 41.0% | 41.2% | 43.3% |
| DS-Coder-Base | 6.7B | 6.7B | 36.1% | 37.5% | 38.2% | 34.0% | 35.0% | 36.2% | 46.8% | 46.4% | 42.9% | 38.8% | 40.8% | 43.3% |
| DS-Coder-Base | 33B | 33B | 39.7% | 40.1% | 40.0% | 36.9% | 38.5% | 39.1% | 47.9% | 47.7% | 43.3% | 40.9% | 43.6% | 44.8% |
| Codestral | 22B | 22B | **42.1%** | **44.3%** | **46.6%** | **46.6%** | **51.5%** | **46.1%** | 48.3% | **47.8%** | **46.0%** | **42.2%** | **43.9%** | **45.7%** |
| DS-Coder-V2-Lite-Base | 16B | 2.4B | 38.3% | 38.6% | 40.6% | 38.3% | 38.7% | 38.9% | **48.8%** | 45.7% | 42.4% | 38.1% | 41.1% | 43.3% |

#### 4.2.2 填充中间代码补全

DeepSeek-Coder-V2-Lite 采用了一种独特的训练方法，在其预训练阶段包含了 0.5 的填充中间（FIM）率。这种方法使模型能够熟练地使用周围上下文（包括前面和后面的代码段）来填充空白，从而完成代码。这种能力对于代码补全工具特别有利。一些开源模型，如 SantaCoder (Allal et al., 2023)、StarCoder (Li et al., 2023b) 和 CodeLlama (Roziere et al., 2023)，也利用了类似的功能，并在代码生成和补全领域树立了高标准。

为了评估 DeepSeek-Coder-V2 模型的性能，我们与领先模型进行了比较分析。评估基于单行填充基准测试，涵盖三种不同的编程语言，如 Allal et al. (2023) 所述。此评估的主要指标是行精确匹配准确率[^fim]。

[^fim]: 我们使用第一行生成的行而不是整个生成的块，因此结果与 DeepSeek-Coder 略有不同。

该表格展示了各种编码模型在 Python、Java 和 JavaScript 三种编程语言的 FIM 任务上的性能，其中平均分表示整体有效性。在比较的模型中，配置了 24 亿激活参数的 DeepSeek-Coder-V2-Lite-Base 取得了出色的结果。它在 Python 上得分为 80.0%，在 Java 上为 89.1%，在 JavaScript 上为 87.2%，从而获得了最高的平均分 86.4%。这证明了 DeepSeek-Coder-V2-Lite-Base 在处理不同编程语言的 FIM 任务方面具有卓越的有效性，在评估中实现了与其他更大模型相当的性能。

表 6：不同方法在 FIM 任务上的性能。

| 模型 | #TP | #AP | python | java | javascript | Mean |
|---|---|---|---|---|---|---|
| StarCoder | 16B | 16B | 71.5% | 82.3% | 83.0% | 80.2% |
| CodeLlama-Base | 7B | 7B | 58.6% | 70.6% | 70.7% | 68.0% |
| CodeLlama-Base | 13B | 13B | 60.7% | 74.3% | 78.5% | 73.1% |
| DS-Coder-Base | 1B | 1B | 74.1% | 85.1% | 82.9% | 81.8% |
| DS-Coder-Base | 7B | 7B | 79.8% | **89.6**% | 86.3% | 86.1% |
| DS-Coder-Base | 33B | 33B | **80.5**% | 88.4% | 86.6% | **86.4**% |
| Codestral | 22B | 22B | 77.2% | 83.2% | 85.9% | 83.0% |
| DS-Coder-V2-Lite-Base | 16B | 2.4B | 80.0% | 89.1% | **87.2**% | **86.4**% |

### 4.3 代码修复

为了评估模型的错误修复能力，我们使用 Defects4J[^defects4j]、SWE-bench (Jimenez et al., 2023) 和 Aider[^aider] 数据集进行测试。Defects4J 是软件工程领域广泛使用的数据集，专门用于评估和测试程序修复技术。它包含来自各种开源项目（包括但不限于 Apache Commons、JFreeChart 和 Closure Compiler）的真实世界软件错误集合。数据集中的每个错误都附带有可用于验证程序修复工具有效性的测试套件。由于 Defects4J 中的原始错误可能需要修改仓库中的多个文件，导致上下文较长，我们从该基准测试中收集了 238 个只需要修改一个方法的错误。

[^defects4j]: https://github.com/riust/defects4j
[^aider]: https://github.com/paul-gauthier/aider

SWE-bench 是一个综合性基准测试，旨在评估大语言模型在解决来自 GitHub 的真实世界软件问题方面的性能。该基准测试呈现一个代码库和一个特定问题，挑战语言模型生成一个有效解决所描述问题的补丁。这个严格的评估框架确保语言模型理解和修复真实世界软件问题的能力得到全面测试，提供了衡量其在实际软件开发任务中的实用性和有效性的清晰标准。

Aider 的代码编辑基准测试评估 LLM 修改 Python 源文件的能力，完成 133 个不同的编码任务。该基准测试不仅测试 LLM 的编码技能，还检查其根据提示词中的规范生成代码编辑的一致性。

表 7 概述了不同语言模型在软件修复基准测试（包括 Defects4J、SWE-Bench 和 Aider）上的性能。在开源模型中，DeepSeek-Coder-Instruct 表现突出，在开源模型中取得了最佳性能。它在 Defects4J 上得分为 21%，在 SWE-Bench 上得分为 12.7%，接近领先闭源模型的结果，并显示出在处理较长代码序列方面的强大能力。值得注意的是，DeepSeek-Coder-V2-Instruct 在 Aider 上取得了 73.7% 的最高分，超过了列表中包括闭源模型在内的所有其他模型。这种卓越的性能突显了其在自动化代码修复任务中的效率和稳健性，使 DeepSeek-Coder-V2-Instruct 成为顶级的开源模型，也是该领域闭源替代方案的有力竞争者。

表 7：不同模型在修复基准测试上的性能。我们没有在 SWE-Bench 上评估 Llama3-Instruct，因为它只支持 8K 上下文长度。

| 模型 | #TP | #AP | Defects4J | SWE-Bench | Aider |
|---|---|---|---|---|---|
| **闭源模型** | | | | | |
| Gemini-1.5-Pro | - | - | 18.6% | 19.3% | 57.1% |
| Claude-3-Opus | - | - | 25.5% | 11.7% | 68.4% |
| GPT-4-1106 | - | - | 22.8% | 22.7% | 65.4% |
| GPT-4-Turbo-0409 | - | - | 24.3% | 18.3% | 63.9% |
| GPT-4o-0513 | - | - | **26.1**% | **26.7**% | **72.9**% |
| **开源模型** | | | | | |
| Codestral | 22B | 22B | 17.8% | 2.7% | 51.1% |
| DS-Coder-Instruct | 33B | 33B | 11.3% | 0.0% | 54.5% |
| Llama3-Instruct | 70B | 70B | 16.2% | - | 49.2% |
| DS-Coder-V2-Lite-Instruct | 16B | 2.4B | 9.2% | 0.0% | 44.4% |
| DS-Coder-V2-Instruct | 236B | 21B | **21.0**% | **12.7**% | **73.7**% |

### 4.4 代码理解与推理

为了评估我们模型的代码推理能力，我们使用了 CRUXEval 基准测试。该基准测试包含 800 个 Python 函数及其对应的输入-输出示例。它分为两个不同的任务：CRUXEval-I，要求大语言模型根据给定的输入预测输出；CRUXEval-O，要求模型从已知的输出预测输入。这种结构挑战了模型在正向和反向两个方向上理解和推理 Python 代码的能力。

表 8 展示了各种语言模型在 CruxEval 基准测试上的性能，该基准测试使用两个指标评估模型：CruxEval-I-COT 和 CruxEval-O-COT。在开源模型中，DeepSeek-Coder-V2-Instruct 表现尤为突出。它在 CruxEval-I-COT 上得分为 70.0%，在 CruxEval-O-COT 上得分为 75.1%，展示了其在开源领域的卓越能力。然而，与更大的闭源模型相比，存在性能差距。这种性能差距可能主要归因于 DeepSeek-Coder-V2-Instruct 仅使用 210 亿激活参数运行，这比 GPT-4o 等更大、更先进的闭源模型中的激活参数要少得多。模型复杂性的这种限制可能会限制其学习和解决问题的能力。

表 8：不同模型在 CruxEval 基准测试上的性能。

| 模型 | #TP | #AP | CruxEval-I-COT | CruxEval-O-COT |
|---|---|---|---|---|
| **闭源模型** | | | | |
| Gemini-1.5-Pro | - | - | 67.0% | 77.5% |
| Claude-3-Opus | - | - | 73.4% | 82.0% |
| GPT-4-1106 | - | - | 75.5% | 77.1% |
| GPT-4-Turbo-0409 | - | - | 75.7% | 82.0% |
| GPT-4o-0513 | - | - | **77.4%** | **88.7%** |
| **开源模型** | | | | |
| Codestral | 22B | 22B | 48.0% | 60.6% |
| DS-Coder-Instruct | 33B | 33B | 47.3% | 50.6% |
| Llama3-Instruct | 70B | 70B | 61.1% | 64.3% |
| DS-Coder-V2-Lite-Instruct | 16B | 2.4B | 53.0% | 52.9% |
| DS-Coder-V2-Instruct | 236B | 21B | **70.0%** | **75.1%** |

### 4.5 数学推理

为了评估 DeepSeekCoder-V2 的数学推理能力，我们使用了流行的小学基准测试 GSM8K (Cobbe et al., 2021)，以及高级竞赛级基准测试，包括 MATH (Hendrycks et al., 2021)、美国数学邀请赛（AIME）2024 (MAA, 2024) 和 Math Odyssey (Netmind.AI, 2024)[^math_bench]。

[^math_bench]: DeepSeek-Coder-V2 在四个数学基准测试上的性能是通过零样本思维链提示获得的；每个测试问题都与指令连接：“\n请逐步推理，并将你的最终答案放在 \boxed{boxed}{} 中。”

表 9 中呈现的结果是在没有借助工具或投票技术的情况下使用贪婪解码获得的（除非另有说明）。DeepSeek-Coder-V2 在 MATH 基准测试上取得了 75.7% 的准确率，在 Math Odyssey 上取得了 53.7% 的准确率，与最先进的 GPT-4o 相当。此外，DeepSeek-Coder-V2 解决了比其它模型更多的 AIME 2024 问题，展示了其强大的数学推理能力。

表 9：不同模型在数学推理方面的性能。DeepSeek-Coder-V2-Instruct 在使用 maj@64 的情况下，在 AIME 2024 上可以达到 5/30。

| 模型 | #TP | #AP | GSM8K | MATH | AIME 2024 | Math Odyssey |
|---|---|---|---|---|---|---|
| **闭源模型** | | | | | | |
| Gemini 1.5 Pro | - | - | 90.8% | 67.7% | 2/30 | 45.0% |
| Claude-3-Opus | - | - | 95.0% | 60.1% | 2/30 | 40.6% |
| GPT-4-1106 | - | - | 91.4% | 64.3% | 1/30 | 49.1% |
| GPT-4-Turbo-0409 | - | - | 93.7% | 73.4% | **3/30** | 46.8% |
| GPT-4o-0513 | - | - | **95.8%** | **76.6%** | 2/30 | **53.2%** |
| **开源模型** | | | | | | |
| Llama3-Instruct | 70B | 70B | 93.0% | 50.4% | 1/30 | 27.9% |
| DS-Coder-V2-Lite-Instruct | 16B | 2.4B | 86.4% | 61.8% | 0/30 | 44.4% |
| DS-Coder-V2-Instruct | 236B | 21B | **94.9%** | **75.7%** | **4/30** | **53.7%** |

### 4.6 通用自然语言

由于 DeepSeek-Coder-V2 是基于 DeepSeek-V2 构建的，它继承了强大的自然语言能力，甚至在推理相关基准测试上超过了 DeepSeek-V2。我们在标准基准测试上将 DeepSeek-Coder-V2 Instruct 与 DeepSeek-V2 Chat 进行比较，这些基准测试涵盖了中英文基准测试，包括 BigBench Hard (BBH) (Suzgun et al., 2022)、MMLU (Hendrycks et al., 2020)、ARC (Clark et al., 2018)、TriviaQA (Joshi et al., 2017)、NaturalQuestions (Kwiatkowski et al., 2019)、AGIEval (Zhong et al., 2023)、CLUEWSC (Xu et al., 2020)、C-Eval (Huang et al., 2023) 和 CMMLU (Li et al., 2023a)。此外，我们还评估了模型的开放式生成能力，包括 Arena-Hard (Li et al., 2024)、AlpacaEval2.0 (Dubois et al., 2024)、MT-Bench (Zheng et al., 2023) 和 Alignbench (Liu et al., 2023c)。评估流程和指标与 DeepSeek-V2 相同，其中 MMLU 使用 OpenAI 的 simple-eval 包进行评估（https://github.com/openai/simple-evals）。

当比较 16B 模型的性能时，可以明显看出 DeepSeek-Coder-V2-Lite-Instruct 在 BBH 和 Arena-Hard 等基准测试上优于 DeepSeek-V2-Lite-Chat。这些基准测试对模型的推理能力要求很高，而这正是 DeepSeek-Coder-V2-Lite-Instruct 擅长的。然而，DeepSeek-Coder-V2-Lite Instruct 在知识密集型基准测试（如 TriviaQA）上落后，这主要是由于预训练期间使用的网络数据量相对较少。

对于 236B 模型，DeepSeek-Coder-V2 Instruct 在推理基准测试（尤其是包含大量代码、数学和推理问题的 Arena-Hard）上表现出更大的优势。另一方面，DeepSeek-V2 Chat 在 MT-bench (Zheng et al., 2023)、AlpacaEval 2.0 (Dubois et al., 2024) 和 AlignBench (Liu et al., 2023c) 等基准测试中表现略好。这种优势可以归因于 DeepSeek-V2 Chat 的通用对齐阶段。

表 10：DeepSeek-Coder-V2 Instruct 与 DeepSeek-V2 Chat 的比较。

| 基准测试（指标） | # Shots | DeepSeek-V2-Lite Chat | DeepSeek-Coder-V2-Lite Instruct | DeepSeek-V2 Chat | DeepSeek-Coder-V2 Instruct |
|---|---|---|---|---|---|
| | | | | | |
| **模型信息** | | | | | |
| # 激活参数 | - | 2.4B | 2.4B | 21B | 21B |
| # 总参数 | - | 16B | 16B | 236B | 236B |
| # 训练 Token 数 | - | 5.7T | 10.2T | 8.1T | 10.2T |
| **英文基准** | | | | | |
| BBH (EM) | 3-shot | 48.1 | **61.2** | 79.7 | **83.9** |
| MMLU (Acc.) | 5-shot | 55.7 | **60.1** | 78.1 | **79.2** |
| ARC-Easy (Acc.) | 25-shot | 56.1 | **88.9** | **98.1** | 97.4 |
| ARC-Challenge (Acc.) | 25-shot | 73.4 | **77.4** | 92.3 | **92.8** |
| TriviaQA (EM) | 5-shot | 65.2 | 59.5 | **86.7** | 82.3 |
| NaturalQuestions (EM) | 5-shot | 35.5 | 30.8 | 53.4 | 47.5 |
| AGIEval (Acc.) | 0-shot | **42.8** | 28.7 | **61.4** | 60.0 |
| **中文基准** | | | | | |
| CLUEWSC (EM) | 5-shot | **80.0** | 76.5 | **89.9** | 85.9 |
| C-Eval (Acc.) | 5-shot | 60.1 | **61.6** | 78.0 | **79.4** |
| CMMLU (Acc.) | 5-shot | 62.5 | **62.7** | **81.6** | 80.9 |
| **开放式生成** | | | | | |
| Arena-Hard | - | 11.40 | **38.10** | 41.60 | **65.00** |
| AlpacaEval 2.0 | - | 16.85 | **17.74** | **38.90** | 36.92 |
| MT-Bench | - | 7.37 | **7.81** | **8.97** | 8.77 |
| Alignbench | - | 6.02 | **6.83** | **7.91** | 7.84 |

## 5 结论

在本文中，我们推出了 DeepSeek-Coder-V2，以进一步推进代码智能领域的发展。它是在 DeepSeek-V2 的基础上，使用来自高质量、多来源语料库的 6 万亿 token 进行持续预训练得到的。通过这种持续预训练，我们发现 DeepSeek-Coder-V2 显著增强了模型的编码和数学推理能力，同时保持了与 DeepSeek-V2 可比的通用语言性能。与 DeepSeek-Coder 相比，DeepSeek-Coder-V2 支持更多的编程语言，从 86 种增加到 338 种，并将最大上下文长度从 16K token 扩展到 128K token。实验结果表明，DeepSeek-Coder-V2 在代码和数学特定任务上取得了与 GPT-4 Turbo、Claude 3 Opus 和 Gemini 1.5 Pro 等最先进闭源模型相当的性能。

尽管 DeepSeek-Coder-V2 在标准基准测试上取得了令人印象深刻的性能，但我们发现，与 GPT-4 Turbo 等当前最先进的模型相比，其在指令遵循能力方面仍存在显著差距。这种差距导致其在复杂场景和任务（如 SWEbench 中的任务）中表现不佳。因此，我们认为代码模型不仅需要强大的编码能力，还需要出色的指令遵循能力来处理现实世界中的复杂编程场景。未来，我们将更多地关注提高模型的指令遵循能力，以更好地处理现实世界中的复杂编程场景，并提高开发过程的生产力。

## 参考文献

（参考文献列表已完整保留，此处省略以节省篇幅）

## 附录 A：支持的编程语言列表

（该列表较长，已完整保留，此处省略以节省篇幅）